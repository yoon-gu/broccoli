{"config":{"lang":["ko","en"],"separator":"[\\s\\-]+","pipeline":[" "]},"docs":[{"location":"","title":"Welcome to Broccoli","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#sortable-table","title":"Sortable Table","text":"Method Description <code>GET</code>      Fetch resource <code>PUT</code>  Update resource <code>DELETE</code>      Delete resource"},{"location":"#sortable-table-from-a-file","title":"Sortable Table from a file","text":"<pre><code>read_csv('assets/basic_table.csv')\n</code></pre> a b 40 73 50 52 531456 80"},{"location":"#tabbed-code-snippets","title":"Tabbed Code Snippets","text":"CC++ <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"#diagrams","title":"Diagrams","text":"Flow ChartSequence DiagramClass DiagramState Diagram <pre><code>graph LR\nA[Start] --&gt; B{Error?};\nB --&gt;|Yes| C[Hmm...];\nC --&gt; D[Debug];\nD --&gt; B;\nB ----&gt;|No| E[Yay!];</code></pre> <pre><code>sequenceDiagram\nautonumber\nAlice-&gt;&gt;John: Hello John, how are you?\nloop Healthcheck\n    John-&gt;&gt;John: Fight against hypochondria\nend\nNote right of John: Rational thoughts!\nJohn--&gt;&gt;Alice: Great!\nJohn-&gt;&gt;Bob: How about you?\nBob--&gt;&gt;John: Jolly good!</code></pre> <pre><code>classDiagram\nPerson &lt;|-- Student\nPerson &lt;|-- Professor\nPerson : +String name\nPerson : +String phoneNumber\nPerson : +String emailAddress\nPerson: +purchaseParkingPass()\nAddress \"1\" &lt;-- \"0..1\" Person:lives at\nclass Student{\n    +int studentNumber\n    +int averageMark\n    +isEligibleToEnrol()\n    +getSeminarsTaken()\n}\nclass Professor{\n    +int salary\n}\nclass Address{\n    +String street\n    +String city\n    +String state\n    +int postalCode\n    +String country\n    -validate()\n    +outputAsLabel()  \n}</code></pre> <pre><code>stateDiagram-v2\nstate fork_state &lt;&lt;fork&gt;&gt;\n    [*] --&gt; fork_state\n    fork_state --&gt; State2\n    fork_state --&gt; State3\n\n    state join_state &lt;&lt;join&gt;&gt;\n    State2 --&gt; join_state\n    State3 --&gt; join_state\n    join_state --&gt; State4\n    State4 --&gt; [*]</code></pre>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"#faq","title":"FAQ","text":"<pre><code>TBA\n</code></pre>"},{"location":"about/","title":"About","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"about/#_1","title":"\ubb38\uc11c\ub97c \ub9c8\ud06c\ub2e4\uc6b4\uc73c\ub85c \ub9cc\ub4dc\ub294 \uc138\uc0c1\uc774\uad70\uc694.","text":"<p>Sphinx\ub3c4 \uc88b\uc558\ub294\ub370, \uc774\uac8c \ub354 \uba85\ud655\ud55c \uac83 \uac19\uae30\ub3c4\ud558\uace0..</p>"},{"location":"about/#open-source","title":"Open Source\uac00 \ub418\uba74\uc11c","text":"<p>\uc138\uc0c1\uc774 \ub9ce\uc774 \ubc14\uaef4\uc11c...ipynb\ub97c \uc5b4\ub5bb\uac8c \uc5f0\ub3d9\uc2dc\ud0a4\ub294\uc9c0 \uc815\ub3c4 \uc54c\uba74 \uc88b\uaca0\ub124\uc694.</p>"},{"location":"about/#_2","title":"\uc608\uc81c \ucf54\ub4dc","text":"<pre><code>import os\nimport torch\nprint(\"hello world.\")\n</code></pre>"},{"location":"horangi/","title":"Horangi \ud55c\uad6d\uc5b4 LLM \ubca4\uce58\ub9c8\ud06c","text":"<p>\ud638\ub791\uc774 LLM \ub9ac\ub354\ubcf4\ub4dc\ub294 \uac70\ub300\uc5b8\uc5b4\ubaa8\ub378(LLM)\uc758 \ud55c\uad6d\uc5b4 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \ub3c4\uad6c\ub85c\uc368 \ub610 \ub2e4\ub978 \ub300\uc548\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc744 \ud1b5\ud574 \ud55c\uad6d\uc5b4\uc5d0 \ub300\ud55c \uc885\ud569\uc801\uc778 \ud3c9\uac00\ub97c \uc218\ud589\ud558\uace0\uc790 \ud569\ub2c8\ub2e4. Q&amp;A \ud615\uc2dd\uc758 \uc5b8\uc5b4\uc774\ud574 llm-kr-eval: \uc77c\ubcf8\uc5b4 \ubc84\uc804\uc778 llm-jp-eval \uae30\ubc18\uc5d0\uc11c \ud55c\uad6d\uc5b4 \ubc84\uc804\uc73c\ub85c \uac1c\ubc1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4. Multi-turn \ub300\ud654\ub97c \ud1b5\ud574 \uc0dd\uc131 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 MT-Bench</p> <p>\ud638\ub791\uc774 LLM \ub9ac\ub354\ubcf4\ub4dc\ub294 Weight &amp; Biases (W&amp;B)\uc758 \ud14c\uc774\ube14 \uae30\ub2a5\uc744 \ud65c\uc6a9\ud558\uc5ec \ud3c9\uac00 \uacb0\uacfc\ub97c \ub2e4\uc591\ud55c \uc2dc\uac01\uc5d0\uc11c \uc27d\uac8c \ubd84\uc11d\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uc774 \uae30\ub2a5\uc744 \ud65c\uc6a9\ud558\uba74 \uac01 \ubaa8\ub378 \ubcc4 \ube44\uad50\ub97c \uc190\uc27d\uac8c \uc218\ud589\ud560 \uc218 \uc788\uc73c\uba70, \uae30\uc874 \uc2e4\ud5d8\ub4e4\uc744 \ucd94\uc801\ud558\uace0 \uae30\ub85d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ubb38\uc11c\ub294 \ud638\ub791\uc774 LLM \ub9ac\ub354\ubcf4\ub4dc\uc758 \ud3c9\uac00 \uacb0\uacfc\uc640, \ud3c9\uac00 \ubc29\ubc95\ub860, \uac01 \ud3c9\uac00 query\uc5d0 \ub300\ud55c \uc138\ubd80 \ubd84\uc11d\uc744 \uc81c\uacf5\ud558\uba70, \uc774\ub97c \ud1b5\ud574 \uc0ac\uc6a9\uc790\uc5d0\uac8c \ucd5c\uc2e0 LLM\uc5d0 \ub300\ud55c \uc774\ud574\ub3c4\ub97c \ub192\uc77c \uc218 \uc788\ub3c4\ub85d \ud558\uace0\uc790 \ud569\ub2c8\ub2e4.</p>"},{"location":"horangi/#radar-plots","title":"Radar Plots","text":"{\"data\":[{\"name\":\"openai\\u002fgpt-4\",\"r\":[9.15,9.75,8.35,7.55,6.25,8.35,9.9,9.25],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"gemini-pro\",\"r\":[8.55,9.9,7.625,6.6,6.4,6.75,9.8,9.5],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"mistral-large\",\"r\":[9.5,9.45,8.8,5.65,6.6,8.35,9.2,9.25],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"yanolja\\u002fEEVE-Korean-Instruct-10.8B-v1.0\",\"r\":[8.1,8.75,7.7,4.4,3.95,5.4,9.3,7.975],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"openai\\u002fgpt-3.5-turbo\",\"r\":[8.3,8.8,8.35,6.2,5.7,7.05,9.65,8.5],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"mistralai\\u002fMixtral-8x7B-Instruct-v0.1\",\"r\":[8.8,8.0,7.9,6.75,4.15,6.55,8.95,7.95],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"nlpai-lab\\u002fKULLM3\",\"r\":[6.5,7.85,7.15,5.3,4.3,4.75,8.9,5.85],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"mistral-medium\",\"r\":[8.7,9.0,8.85,5.5,5.5,6.8,8.2,9.2],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"anthropic.claude-v2:1\",\"r\":[6.35,9.2,6.95,6.2,6.75,6.85,8.45,8.95],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"Edentns\\u002fDataVortexS-10.7B-dpo-v1.11\",\"r\":[5.0,5.9,6.05,4.55,2.5,4.95,8.15,4.95],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"ONS-SOLAR-10.7B-v1.2\",\"r\":[3.45,7.25,6.6,3.3,2.7,2.95,6.15,5.15],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"Raphael21\\u002fRaphael21-SOLAR-10.7B\",\"r\":[4.7,4.65,6.25,2.75,1.8,4.5,5.3,4.35],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"LDCC-SOLAR-10.7B\",\"r\":[6.25,5.3,6.35,3.2,2.0,4.45,6.9,3.65],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"beomi\\u002fSOLAR-KOEN-10.8B\",\"r\":[4.0,4.55,4.7,2.4,1.35,1.8,5.7,2.3],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"beomi\\u002fOPEN-SOLAR-KO-10.7B\",\"r\":[3.05,2.05,3.1,3.55,1.5,1.1,2.35,3.3],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"KT-AI\\u002fmidm-bitext-S-7B-inst-v1\",\"r\":[0.9,0.9,1.0,1.0,1.0,1.0,1.0,1.0],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"42dot\\u002f42dot_LLM-SFT-1.3B\",\"r\":[3.25,2.4,3.85,1.1,1.2,1.15,1.75,2.0],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"nlpai-lab\\u002fkullm-polyglot-5.8b-v2\",\"r\":[1.0,2.0,1.95,1.4,1.25,1.3,1.2,1.5],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"beomi\\u002fgemma-ko-7b\",\"r\":[1.05,1.15,1.55,1.0,0.85,0.9,1.2,1.05],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"nlpai-lab\\u002fkullm-polyglot-12.8b-v2\",\"r\":[0.9,0.9,0.9,0.85,0.6,0.7,0.75,0.95],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"EleutherAI\\u002fpolyglot-ko-5.8b\",\"r\":[0.75,1.05,1.7,1.0,1.4,0.95,1.05,1.0],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"megastudyedu\\u002fM-SOLAR-10.7B-v1.4-dpo\",\"r\":[0.95,0.9,0.8,1.3,0.95,1.0,0.8,0.9],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"},{\"name\":\"EleutherAI\\u002fpolyglot-ko-12.8b\",\"r\":[0.9,0.9,0.9,0.75,0.8,1.0,0.85,0.75],\"theta\":[\"writing\",\"stem\",\"roleplay\",\"reasoning\",\"math\",\"coding\",\"humanities\",\"extraction\"],\"type\":\"scatterpolar\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}}}"},{"location":"horangi/#benchmarks","title":"Benchmarks","text":"AVGAbilitiesEL,FA,NLI,QA,RCALL model name AVG AVG_llm_kr_eval AVG_mtbench openai/gpt-4 0.74 0.62 8.57 gemini-pro 0.66 0.52 8.14 mistral-large 0.63 0.42 8.35 yanolja/EEVE-Korean-Instruct-10.8B-v1.0 0.61 0.53 6.95 openai/gpt-3.5-turbo 0.60 0.42 7.82 mistralai/Mixtral-8x7B-Instruct-v0.1 0.58 0.43 7.38 nlpai-lab/KULLM3 0.58 0.52 6.32 mistral-medium 0.55 0.34 7.72 anthropic.claude-v2:1 0.51 0.27 7.46 Edentns/DataVortexS-10.7B-dpo-v1.11 0.46 0.40 5.26 ONS-SOLAR-10.7B-v1.2 0.41 0.36 4.69 Raphael21/Raphael21-SOLAR-10.7B 0.40 0.38 4.29 LDCC-SOLAR-10.7B 0.39 0.30 4.76 beomi/SOLAR-KOEN-10.8B 0.30 0.26 3.35 beomi/OPEN-SOLAR-KO-10.7B 0.25 0.24 2.50 KT-AI/midm-bitext-S-7B-inst-v1 0.23 0.36 0.97 42dot/42dot_LLM-SFT-1.3B 0.16 0.10 2.09 nlpai-lab/kullm-polyglot-5.8b-v2 0.13 0.12 1.45 beomi/gemma-ko-7b 0.11 0.10 1.09 nlpai-lab/kullm-polyglot-12.8b-v2 0.09 0.11 0.82 EleutherAI/polyglot-ko-5.8b 0.06 0.01 1.11 megastudyedu/M-SOLAR-10.7B-v1.4-dpo 0.05 0.01 0.95 EleutherAI/polyglot-ko-12.8b 0.05 0.02 0.86 model name coding extraction humanities math reasoning roleplay stem writing openai/gpt-4 8.35 9.25 9.90 6.25 7.55 8.35 9.75 9.15 gemini-pro 6.75 9.50 9.80 6.40 6.60 7.62 9.90 8.55 mistral-large 8.35 9.25 9.20 6.60 5.65 8.80 9.45 9.50 yanolja/EEVE-Korean-Instruct-10.8B-v1.0 5.40 7.97 9.30 3.95 4.40 7.70 8.75 8.10 openai/gpt-3.5-turbo 7.05 8.50 9.65 5.70 6.20 8.35 8.80 8.30 mistralai/Mixtral-8x7B-Instruct-v0.1 6.55 7.95 8.95 4.15 6.75 7.90 8.00 8.80 nlpai-lab/KULLM3 4.75 5.85 8.90 4.30 5.30 7.15 7.85 6.50 mistral-medium 6.80 9.20 8.20 5.50 5.50 8.85 9.00 8.70 anthropic.claude-v2:1 6.85 8.95 8.45 6.75 6.20 6.95 9.20 6.35 Edentns/DataVortexS-10.7B-dpo-v1.11 4.95 4.95 8.15 2.50 4.55 6.05 5.90 5.00 ONS-SOLAR-10.7B-v1.2 2.95 5.15 6.15 2.70 3.30 6.60 7.25 3.45 Raphael21/Raphael21-SOLAR-10.7B 4.50 4.35 5.30 1.80 2.75 6.25 4.65 4.70 LDCC-SOLAR-10.7B 4.45 3.65 6.90 2.00 3.20 6.35 5.30 6.25 beomi/SOLAR-KOEN-10.8B 1.80 2.30 5.70 1.35 2.40 4.70 4.55 4.00 beomi/OPEN-SOLAR-KO-10.7B 1.10 3.30 2.35 1.50 3.55 3.10 2.05 3.05 KT-AI/midm-bitext-S-7B-inst-v1 1.00 1.00 1.00 1.00 1.00 1.00 0.90 0.90 42dot/42dot_LLM-SFT-1.3B 1.15 2.00 1.75 1.20 1.10 3.85 2.40 3.25 nlpai-lab/kullm-polyglot-5.8b-v2 1.30 1.50 1.20 1.25 1.40 1.95 2.00 1.00 beomi/gemma-ko-7b 0.90 1.05 1.20 0.85 1.00 1.55 1.15 1.05 nlpai-lab/kullm-polyglot-12.8b-v2 0.70 0.95 0.75 0.60 0.85 0.90 0.90 0.90 EleutherAI/polyglot-ko-5.8b 0.95 1.00 1.05 1.40 1.00 1.70 1.05 0.75 megastudyedu/M-SOLAR-10.7B-v1.4-dpo 1.00 0.90 0.80 0.95 1.30 0.80 0.90 0.95 EleutherAI/polyglot-ko-12.8b 1.00 0.75 0.85 0.80 0.75 0.90 0.90 0.90 model name EL FA NLI QA RC openai/gpt-4 0.28 0.40 0.83 0.65 0.92 gemini-pro 0.20 0.23 0.72 0.52 0.91 mistral-large 0.34 0.38 0.43 0.00 0.93 yanolja/EEVE-Korean-Instruct-10.8B-v1.0 0.14 0.35 0.75 0.51 0.92 openai/gpt-3.5-turbo 0.10 0.44 0.44 0.26 0.87 mistralai/Mixtral-8x7B-Instruct-v0.1 0.04 0.26 0.67 0.32 0.86 nlpai-lab/KULLM3 0.03 0.42 0.75 0.51 0.90 mistral-medium 0.05 0.32 0.37 0.04 0.90 anthropic.claude-v2:1 0.01 0.25 0.45 0.39 0.26 Edentns/DataVortexS-10.7B-dpo-v1.11 0.10 0.08 0.70 0.23 0.91 ONS-SOLAR-10.7B-v1.2 0.07 0.12 0.61 0.31 0.68 Raphael21/Raphael21-SOLAR-10.7B 0.08 0.06 0.65 0.29 0.81 LDCC-SOLAR-10.7B 0.00 0.08 0.52 0.32 0.60 beomi/SOLAR-KOEN-10.8B 0.00 0.11 0.49 0.20 0.46 beomi/OPEN-SOLAR-KO-10.7B 0.04 0.34 0.42 0.09 0.33 KT-AI/midm-bitext-S-7B-inst-v1 0.00 0.29 0.59 0.33 0.57 42dot/42dot_LLM-SFT-1.3B 0.00 0.19 0.29 0.08 -0.05 nlpai-lab/kullm-polyglot-5.8b-v2 0.00 0.30 0.26 0.03 0.00 beomi/gemma-ko-7b 0.00 0.23 0.19 0.05 0.03 nlpai-lab/kullm-polyglot-12.8b-v2 0.00 0.21 0.21 0.12 0.00 EleutherAI/polyglot-ko-5.8b 0.00 0.00 0.04 0.00 0.00 megastudyedu/M-SOLAR-10.7B-v1.4-dpo 0.00 0.00 0.00 0.00 0.05 EleutherAI/polyglot-ko-12.8b 0.00 0.01 0.00 0.00 0.09 model name AVG AVG_llm_kr_eval AVG_mtbench EL FA NLI QA RC klue_ner_set_f1 klue_re_exact_match kmmlu_preview_exact_match kobest_copa_exact_match kobest_hs_exact_match kobest_sn_exact_match kobest_wic_exact_match korea_cg_bleu kornli_exact_match korsts_pearson korsts_spearman coding extraction humanities math reasoning roleplay stem writing openai/gpt-4 0.74 0.62 8.57 0.28 0.40 0.83 0.65 0.92 0.12 0.44 0.46 1.00 0.74 1.00 0.83 0.40 0.76 0.88 0.89 8.35 9.25 9.90 6.25 7.55 8.35 9.75 9.15 gemini-pro 0.66 0.52 8.14 0.20 0.23 0.72 0.52 0.91 0.09 0.30 0.42 0.94 0.52 1.00 0.61 0.23 0.71 0.87 0.87 6.75 9.50 9.80 6.40 6.60 7.62 9.90 8.55 mistral-large 0.63 0.42 8.35 0.34 0.38 0.43 0.00 0.93 0.11 0.58 0.00 0.55 0.03 0.99 0.00 0.38 0.72 0.90 0.90 8.35 9.25 9.20 6.60 5.65 8.80 9.45 9.50 yanolja/EEVE-Korean-Instruct-10.8B-v1.0 0.61 0.53 6.95 0.14 0.35 0.75 0.51 0.92 0.11 0.17 0.37 0.98 0.51 0.99 0.64 0.35 0.77 0.89 0.89 5.40 7.97 9.30 3.95 4.40 7.70 8.75 8.10 openai/gpt-3.5-turbo 0.60 0.42 7.82 0.10 0.44 0.44 0.26 0.87 0.03 0.17 0.00 0.34 0.47 0.95 0.52 0.44 0.51 0.83 0.83 7.05 8.50 9.65 5.70 6.20 8.35 8.80 8.30 mistralai/Mixtral-8x7B-Instruct-v0.1 0.58 0.43 7.38 0.04 0.26 0.67 0.32 0.86 0.01 0.07 0.23 0.87 0.48 0.98 0.41 0.26 0.65 0.80 0.81 6.55 7.95 8.95 4.15 6.75 7.90 8.00 8.80 nlpai-lab/KULLM3 0.58 0.52 6.32 0.03 0.42 0.75 0.51 0.90 0.00 0.06 0.36 0.93 0.65 0.98 0.65 0.42 0.66 0.87 0.85 4.75 5.85 8.90 4.30 5.30 7.15 7.85 6.50 mistral-medium 0.55 0.34 7.72 0.05 0.32 0.37 0.04 0.90 0.06 0.04 0.01 0.40 0.25 0.96 0.08 0.32 0.46 0.87 0.86 6.80 9.20 8.20 5.50 5.50 8.85 9.00 8.70 anthropic.claude-v2:1 0.51 0.27 7.46 0.01 0.25 0.45 0.39 0.26 0.01 0.00 0.25 0.72 0.12 0.92 0.52 0.25 0.52 -0.09 -0.04 6.85 8.95 8.45 6.75 6.20 6.95 9.20 6.35 Edentns/DataVortexS-10.7B-dpo-v1.11 0.46 0.40 5.26 0.10 0.08 0.70 0.23 0.91 0.00 0.21 0.30 0.93 0.52 0.95 0.17 0.08 0.64 0.89 0.88 4.95 4.95 8.15 2.50 4.55 6.05 5.90 5.00 ONS-SOLAR-10.7B-v1.2 0.41 0.36 4.69 0.07 0.12 0.61 0.31 0.68 0.00 0.14 0.39 0.95 0.44 0.25 0.23 0.12 0.45 0.88 0.90 2.95 5.15 6.15 2.70 3.30 6.60 7.25 3.45 Raphael21/Raphael21-SOLAR-10.7B 0.40 0.38 4.29 0.08 0.06 0.65 0.29 0.81 0.00 0.16 0.37 0.91 0.39 0.60 0.21 0.06 0.65 0.92 0.92 4.50 4.35 5.30 1.80 2.75 6.25 4.65 4.70 LDCC-SOLAR-10.7B 0.39 0.30 4.76 0.00 0.08 0.52 0.32 0.60 0.00 0.00 0.36 0.92 0.55 0.00 0.27 0.08 0.09 0.91 0.90 4.45 3.65 6.90 2.00 3.20 6.35 5.30 6.25 beomi/SOLAR-KOEN-10.8B 0.30 0.26 3.35 0.00 0.11 0.49 0.20 0.46 0.00 0.00 0.19 0.74 0.42 0.00 0.22 0.11 0.32 0.68 0.71 1.80 2.30 5.70 1.35 2.40 4.70 4.55 4.00 beomi/OPEN-SOLAR-KO-10.7B 0.25 0.24 2.50 0.04 0.34 0.42 0.09 0.33 0.02 0.05 0.18 0.54 0.40 0.27 0.00 0.34 0.33 0.36 0.36 1.10 3.30 2.35 1.50 3.55 3.10 2.05 3.05 KT-AI/midm-bitext-S-7B-inst-v1 0.23 0.36 0.97 0.00 0.29 0.59 0.33 0.57 0.00 0.00 0.20 0.80 0.44 0.62 0.45 0.29 0.54 0.55 0.55 1.00 1.00 1.00 1.00 1.00 1.00 0.90 0.90 42dot/42dot_LLM-SFT-1.3B 0.16 0.10 2.09 0.00 0.19 0.29 0.08 -0.05 0.00 0.00 0.16 0.51 0.12 0.04 0.00 0.19 0.24 -0.10 -0.09 1.15 2.00 1.75 1.20 1.10 3.85 2.40 3.25 nlpai-lab/kullm-polyglot-5.8b-v2 0.13 0.12 1.45 0.00 0.30 0.26 0.03 0.00 0.00 0.00 0.05 0.51 0.27 0.00 0.00 0.30 0.00 0.00 0.00 1.30 1.50 1.20 1.25 1.40 1.95 2.00 1.00 beomi/gemma-ko-7b 0.11 0.10 1.09 0.00 0.23 0.19 0.05 0.03 0.00 0.00 0.10 0.30 0.27 0.10 0.00 0.23 0.00 0.00 0.00 0.90 1.05 1.20 0.85 1.00 1.55 1.15 1.05 nlpai-lab/kullm-polyglot-12.8b-v2 0.09 0.11 0.82 0.00 0.21 0.21 0.12 0.00 0.00 0.00 0.23 0.43 0.21 0.00 0.00 0.21 0.00 0.00 0.00 0.70 0.95 0.75 0.60 0.85 0.90 0.90 0.90 EleutherAI/polyglot-ko-5.8b 0.06 0.01 1.11 0.00 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.10 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.95 1.00 1.05 1.40 1.00 1.70 1.05 0.75 megastudyedu/M-SOLAR-10.7B-v1.4-dpo 0.05 0.01 0.95 0.00 0.00 0.00 0.00 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.08 0.07 1.00 0.90 0.80 0.95 1.30 0.80 0.90 0.95 EleutherAI/polyglot-ko-12.8b 0.05 0.02 0.86 0.00 0.01 0.00 0.00 0.09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.13 0.14 1.00 0.75 0.85 0.80 0.75 0.90 0.90 0.90"},{"location":"examples/","title":"Examples","text":"<p>In the navigation to the left, you will find many example notebooks, displaying the usage of various llama-index components and use-cases.</p> <pre><code>import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport pandas as pd\nimport re\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/Mining-BTC-180.csv\")\n\nfor i, row in enumerate(df[\"Date\"]):\n    p = re.compile(\" 00:00:00\")\n    datetime = p.split(df[\"Date\"][i])[0]\n    df.iloc[i, 1] = datetime\n\nfig = make_subplots(\n    rows=3, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.03,\n    specs=[[{\"type\": \"table\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=df[\"Date\"],\n        y=df[\"Mining-revenue-USD\"],\n        mode=\"lines\",\n        name=\"mining revenue\"\n    ),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=df[\"Date\"],\n        y=df[\"Hash-rate\"],\n        mode=\"lines\",\n        name=\"hash-rate-TH/s\"\n    ),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=[\"Date\", \"Number&lt;br&gt;Transactions\", \"Output&lt;br&gt;Volume (BTC)\",\n                    \"Market&lt;br&gt;Price\", \"Hash&lt;br&gt;Rate\", \"Cost per&lt;br&gt;trans-USD\",\n                    \"Mining&lt;br&gt;Revenue-USD\", \"Trasaction&lt;br&gt;fees-BTC\"],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[df[k].tolist() for k in df.columns[1:]],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Bitcoin mining stats for 180 days\",\n)\n\nwith open(\"./docs/assets/test.json\", \"w\") as f:\n    f.write(fig.to_json())\n</code></pre> {\"data\":[{\"mode\":\"lines\",\"name\":\"mining revenue\",\"x\":[\"2017-04-29\",\"2017-04-30\",\"2017-05-01\",\"2017-05-02\",\"2017-05-03\",\"2017-05-04\",\"2017-05-05\",\"2017-05-06\",\"2017-05-07\",\"2017-05-08\",\"2017-05-09\",\"2017-05-10\",\"2017-05-11\",\"2017-05-12\",\"2017-05-13\",\"2017-05-14\",\"2017-05-15\",\"2017-05-16\",\"2017-05-17\",\"2017-05-18\",\"2017-05-19\",\"2017-05-20\",\"2017-05-21\",\"2017-05-22\",\"2017-05-23\",\"2017-05-24\",\"2017-05-25\",\"2017-05-26\",\"2017-05-27\",\"2017-05-28\",\"2017-05-29\",\"2017-05-30\",\"2017-05-31\",\"2017-06-01\",\"2017-06-02\",\"2017-06-03\",\"2017-06-04\",\"2017-06-05\",\"2017-06-06\",\"2017-06-07\",\"2017-06-08\",\"2017-06-09\",\"2017-06-10\",\"2017-06-11\",\"2017-06-12\",\"2017-06-13\",\"2017-06-14\",\"2017-06-15\",\"2017-06-16\",\"2017-06-17\",\"2017-06-18\",\"2017-06-19\",\"2017-06-20\",\"2017-06-21\",\"2017-06-22\",\"2017-06-23\",\"2017-06-24\",\"2017-06-25\",\"2017-06-26\",\"2017-06-27\",\"2017-06-28\",\"2017-06-29\",\"2017-06-30\",\"2017-07-01\",\"2017-07-02\",\"2017-07-03\",\"2017-07-04\",\"2017-07-05\",\"2017-07-06\",\"2017-07-07\",\"2017-07-08\",\"2017-07-09\",\"2017-07-10\",\"2017-07-11\",\"2017-07-12\",\"2017-07-13\",\"2017-07-14\",\"2017-07-15\",\"2017-07-16\",\"2017-07-17\",\"2017-07-18\",\"2017-07-19\",\"2017-07-20\",\"2017-07-21\",\"2017-07-22\",\"2017-07-23\",\"2017-07-24\",\"2017-07-25\",\"2017-07-26\",\"2017-07-27\",\"2017-07-28\",\"2017-07-29\",\"2017-07-30\",\"2017-07-31\",\"2017-08-01\",\"2017-08-02\",\"2017-08-03\",\"2017-08-04\",\"2017-08-05\",\"2017-08-06\",\"2017-08-07\",\"2017-08-08\",\"2017-08-09\",\"2017-08-10\",\"2017-08-11\",\"2017-08-12\",\"2017-08-13\",\"2017-08-14\",\"2017-08-15\",\"2017-08-16\",\"2017-08-17\",\"2017-08-18\",\"2017-08-19\",\"2017-08-20\",\"2017-08-21\",\"2017-08-22\",\"2017-08-23\",\"2017-08-24\",\"2017-08-25\",\"2017-08-26\",\"2017-08-27\",\"2017-08-28\",\"2017-08-29\",\"2017-08-30\",\"2017-08-31\",\"2017-09-01\",\"2017-09-02\",\"2017-09-03\",\"2017-09-04\",\"2017-09-05\",\"2017-09-06\",\"2017-09-07\",\"2017-09-08\",\"2017-09-09\",\"2017-09-10\",\"2017-09-11\",\"2017-09-12\",\"2017-09-13\",\"2017-09-14\",\"2017-09-15\",\"2017-09-16\",\"2017-09-17\",\"2017-09-18\",\"2017-09-19\",\"2017-09-20\",\"2017-09-21\",\"2017-09-22\",\"2017-09-23\",\"2017-09-24\",\"2017-09-25\",\"2017-09-26\",\"2017-09-27\",\"2017-09-28\",\"2017-09-29\",\"2017-09-30\",\"2017-10-01\",\"2017-10-02\",\"2017-10-03\",\"2017-10-04\",\"2017-10-05\",\"2017-10-06\",\"2017-10-07\",\"2017-10-08\",\"2017-10-09\",\"2017-10-10\",\"2017-10-11\",\"2017-10-12\",\"2017-10-13\",\"2017-10-14\",\"2017-10-15\",\"2017-10-16\",\"2017-10-17\",\"2017-10-18\",\"2017-10-19\",\"2017-10-20\",\"2017-10-21\",\"2017-10-22\",\"2017-10-23\",\"2017-10-24\"],\"y\":[3119179,2720216,2878278,3149553,2760373,3500746,2779170,3610789,3505620,4062173,3715394,3665041,3468077,3731718,3885821,4018028,4090703,3356332,4241264,3986175,4746880,4862317,4331490,4499105,5668946,5524488,5871334,5119620,5121737,5262057,5260814,6132064,5808558,6245229,5437775,7190853,5967105,6586584,6705146,7060708,6905690,6408509,5916953,6255384,5779114,5967684,5217093,5540766,5304594,5461151,5066180,5761788,5639440,5729388,5791396,6125532,5333680,4323782,5067818,4893545,5859694,4529058,5463199,5415823,6165437,5975339,5536125,5284211,5092047,5027244,5589288,4618491,6117733,5418690,5595773,4943868,4133223,3915180,4114452,4307538,5135591,4503487,6303560,5613835,5890365,5998935,6381473,4978580,4829128,5348203,5563273,5536230,5226926,5550845,5102353,5295684,5009853,6553817,7255240,5802572,6700312,7395324,6117049,6163928,7130611,7067967,8147957,7782196,9146734,8320553,10849951,8775120,8768358,7392849,6787387,5749752,10230715,6929316,7153228,7319122,9642616,8451776,10384095,11046351,11260487,12131998,10374578,9700710,10527922,11928774,10290919,11050159,10028960,9554034,7845878,9592187,10193192,9617967,7489085,8227898,8161423,8368700,6960685,8091884,6147322,8287404,6520223,7983282,6815446,6522957,7065925,8033184,9528010,9047637,7690943,7369622,8729899,10134350,7436308,7204664,8954116,8787393,10645076,9983564,10009312,8773874,10754101,10610179,11022592,14217107,13050105,12204887,11106218,13368297,13875337,12037733,14108349,13224118,12165126],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines\",\"name\":\"hash-rate-TH\\u002fs\",\"x\":[\"2017-04-29\",\"2017-04-30\",\"2017-05-01\",\"2017-05-02\",\"2017-05-03\",\"2017-05-04\",\"2017-05-05\",\"2017-05-06\",\"2017-05-07\",\"2017-05-08\",\"2017-05-09\",\"2017-05-10\",\"2017-05-11\",\"2017-05-12\",\"2017-05-13\",\"2017-05-14\",\"2017-05-15\",\"2017-05-16\",\"2017-05-17\",\"2017-05-18\",\"2017-05-19\",\"2017-05-20\",\"2017-05-21\",\"2017-05-22\",\"2017-05-23\",\"2017-05-24\",\"2017-05-25\",\"2017-05-26\",\"2017-05-27\",\"2017-05-28\",\"2017-05-29\",\"2017-05-30\",\"2017-05-31\",\"2017-06-01\",\"2017-06-02\",\"2017-06-03\",\"2017-06-04\",\"2017-06-05\",\"2017-06-06\",\"2017-06-07\",\"2017-06-08\",\"2017-06-09\",\"2017-06-10\",\"2017-06-11\",\"2017-06-12\",\"2017-06-13\",\"2017-06-14\",\"2017-06-15\",\"2017-06-16\",\"2017-06-17\",\"2017-06-18\",\"2017-06-19\",\"2017-06-20\",\"2017-06-21\",\"2017-06-22\",\"2017-06-23\",\"2017-06-24\",\"2017-06-25\",\"2017-06-26\",\"2017-06-27\",\"2017-06-28\",\"2017-06-29\",\"2017-06-30\",\"2017-07-01\",\"2017-07-02\",\"2017-07-03\",\"2017-07-04\",\"2017-07-05\",\"2017-07-06\",\"2017-07-07\",\"2017-07-08\",\"2017-07-09\",\"2017-07-10\",\"2017-07-11\",\"2017-07-12\",\"2017-07-13\",\"2017-07-14\",\"2017-07-15\",\"2017-07-16\",\"2017-07-17\",\"2017-07-18\",\"2017-07-19\",\"2017-07-20\",\"2017-07-21\",\"2017-07-22\",\"2017-07-23\",\"2017-07-24\",\"2017-07-25\",\"2017-07-26\",\"2017-07-27\",\"2017-07-28\",\"2017-07-29\",\"2017-07-30\",\"2017-07-31\",\"2017-08-01\",\"2017-08-02\",\"2017-08-03\",\"2017-08-04\",\"2017-08-05\",\"2017-08-06\",\"2017-08-07\",\"2017-08-08\",\"2017-08-09\",\"2017-08-10\",\"2017-08-11\",\"2017-08-12\",\"2017-08-13\",\"2017-08-14\",\"2017-08-15\",\"2017-08-16\",\"2017-08-17\",\"2017-08-18\",\"2017-08-19\",\"2017-08-20\",\"2017-08-21\",\"2017-08-22\",\"2017-08-23\",\"2017-08-24\",\"2017-08-25\",\"2017-08-26\",\"2017-08-27\",\"2017-08-28\",\"2017-08-29\",\"2017-08-30\",\"2017-08-31\",\"2017-09-01\",\"2017-09-02\",\"2017-09-03\",\"2017-09-04\",\"2017-09-05\",\"2017-09-06\",\"2017-09-07\",\"2017-09-08\",\"2017-09-09\",\"2017-09-10\",\"2017-09-11\",\"2017-09-12\",\"2017-09-13\",\"2017-09-14\",\"2017-09-15\",\"2017-09-16\",\"2017-09-17\",\"2017-09-18\",\"2017-09-19\",\"2017-09-20\",\"2017-09-21\",\"2017-09-22\",\"2017-09-23\",\"2017-09-24\",\"2017-09-25\",\"2017-09-26\",\"2017-09-27\",\"2017-09-28\",\"2017-09-29\",\"2017-09-30\",\"2017-10-01\",\"2017-10-02\",\"2017-10-03\",\"2017-10-04\",\"2017-10-05\",\"2017-10-06\",\"2017-10-07\",\"2017-10-08\",\"2017-10-09\",\"2017-10-10\",\"2017-10-11\",\"2017-10-12\",\"2017-10-13\",\"2017-10-14\",\"2017-10-15\",\"2017-10-16\",\"2017-10-17\",\"2017-10-18\",\"2017-10-19\",\"2017-10-20\",\"2017-10-21\",\"2017-10-22\",\"2017-10-23\",\"2017-10-24\"],\"y\":[4488916,3918072,3892124,4099704,3425069,4359179,3347227,4359179,4333232,4670549,4021862,4119773,3757901,4286791,4398136,4565154,4732172,3674392,4314627,3952755,4565154,4592990,4091937,4064100,4976738,4562010,4828621,4532386,4976738,4887867,4473139,5302596,4858244,5035985,4325022,5687700,5432357,5533581,5094944,5567322,5567322,5094944,4757530,4926237,4959978,4757530,4723789,5162426,4993720,5129906,5129906,5342178,4882255,4953012,4917634,5660586,5165284,4316196,5023770,4634604,5554450,4245439,5448314,5802100,6622799,5988701,5389831,5248920,5002327,5213693,5812563,4861416,6939848,6094384,6235295,5530742,5559050,5079132,6518886,5918988,6598872,5958981,6478893,6158947,6398906,6878824,7118783,5918988,5878995,6585327,6414280,6713613,6285994,6200471,6371518,6499804,5901138,7654374,7568850,5986661,6542566,7012946,6333395,6149818,6608760,6195712,6792336,5966241,6975913,5966241,8215055,6838231,6930019,5966241,5277829,4084581,7297172,4459278,4459278,4635883,6843446,5695513,6578538,7417412,7196656,7682320,7240807,6799294,7726471,8697799,7430761,8072925,8027056,7751843,6329907,7981187,8623352,8715090,7981187,8256401,7751843,8072925,6965995,8556656,6417492,9653663,7624200,9105159,7953302,6856295,7569349,8063003,9708513,9269710,7569349,7130547,8380132,10223761,7486251,6983443,8882940,8827072,10167894,8994675,8938808,7765589,8547735,7597986,7877324,11601113,10351762,9756834,8983426,10589734,10649227,9102412,10827706,10173284,9935312],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"cells\":{\"align\":\"left\",\"values\":[[\"2017-04-29\",\"2017-04-30\",\"2017-05-01\",\"2017-05-02\",\"2017-05-03\",\"2017-05-04\",\"2017-05-05\",\"2017-05-06\",\"2017-05-07\",\"2017-05-08\",\"2017-05-09\",\"2017-05-10\",\"2017-05-11\",\"2017-05-12\",\"2017-05-13\",\"2017-05-14\",\"2017-05-15\",\"2017-05-16\",\"2017-05-17\",\"2017-05-18\",\"2017-05-19\",\"2017-05-20\",\"2017-05-21\",\"2017-05-22\",\"2017-05-23\",\"2017-05-24\",\"2017-05-25\",\"2017-05-26\",\"2017-05-27\",\"2017-05-28\",\"2017-05-29\",\"2017-05-30\",\"2017-05-31\",\"2017-06-01\",\"2017-06-02\",\"2017-06-03\",\"2017-06-04\",\"2017-06-05\",\"2017-06-06\",\"2017-06-07\",\"2017-06-08\",\"2017-06-09\",\"2017-06-10\",\"2017-06-11\",\"2017-06-12\",\"2017-06-13\",\"2017-06-14\",\"2017-06-15\",\"2017-06-16\",\"2017-06-17\",\"2017-06-18\",\"2017-06-19\",\"2017-06-20\",\"2017-06-21\",\"2017-06-22\",\"2017-06-23\",\"2017-06-24\",\"2017-06-25\",\"2017-06-26\",\"2017-06-27\",\"2017-06-28\",\"2017-06-29\",\"2017-06-30\",\"2017-07-01\",\"2017-07-02\",\"2017-07-03\",\"2017-07-04\",\"2017-07-05\",\"2017-07-06\",\"2017-07-07\",\"2017-07-08\",\"2017-07-09\",\"2017-07-10\",\"2017-07-11\",\"2017-07-12\",\"2017-07-13\",\"2017-07-14\",\"2017-07-15\",\"2017-07-16\",\"2017-07-17\",\"2017-07-18\",\"2017-07-19\",\"2017-07-20\",\"2017-07-21\",\"2017-07-22\",\"2017-07-23\",\"2017-07-24\",\"2017-07-25\",\"2017-07-26\",\"2017-07-27\",\"2017-07-28\",\"2017-07-29\",\"2017-07-30\",\"2017-07-31\",\"2017-08-01\",\"2017-08-02\",\"2017-08-03\",\"2017-08-04\",\"2017-08-05\",\"2017-08-06\",\"2017-08-07\",\"2017-08-08\",\"2017-08-09\",\"2017-08-10\",\"2017-08-11\",\"2017-08-12\",\"2017-08-13\",\"2017-08-14\",\"2017-08-15\",\"2017-08-16\",\"2017-08-17\",\"2017-08-18\",\"2017-08-19\",\"2017-08-20\",\"2017-08-21\",\"2017-08-22\",\"2017-08-23\",\"2017-08-24\",\"2017-08-25\",\"2017-08-26\",\"2017-08-27\",\"2017-08-28\",\"2017-08-29\",\"2017-08-30\",\"2017-08-31\",\"2017-09-01\",\"2017-09-02\",\"2017-09-03\",\"2017-09-04\",\"2017-09-05\",\"2017-09-06\",\"2017-09-07\",\"2017-09-08\",\"2017-09-09\",\"2017-09-10\",\"2017-09-11\",\"2017-09-12\",\"2017-09-13\",\"2017-09-14\",\"2017-09-15\",\"2017-09-16\",\"2017-09-17\",\"2017-09-18\",\"2017-09-19\",\"2017-09-20\",\"2017-09-21\",\"2017-09-22\",\"2017-09-23\",\"2017-09-24\",\"2017-09-25\",\"2017-09-26\",\"2017-09-27\",\"2017-09-28\",\"2017-09-29\",\"2017-09-30\",\"2017-10-01\",\"2017-10-02\",\"2017-10-03\",\"2017-10-04\",\"2017-10-05\",\"2017-10-06\",\"2017-10-07\",\"2017-10-08\",\"2017-10-09\",\"2017-10-10\",\"2017-10-11\",\"2017-10-12\",\"2017-10-13\",\"2017-10-14\",\"2017-10-15\",\"2017-10-16\",\"2017-10-17\",\"2017-10-18\",\"2017-10-19\",\"2017-10-20\",\"2017-10-21\",\"2017-10-22\",\"2017-10-23\",\"2017-10-24\"],[341319,281489,294786,333161,295149,354737,267193,363022,316011,365096,332879,311391,294743,317698,329266,369098,329229,233977,317527,288904,319502,352805,326057,327868,367710,338642,350114,333340,331914,308143,321638,347961,321634,319709,271539,305320,266044,289930,297416,339720,307377,282184,254993,222892,269098,291776,287644,293141,269937,236554,209359,269774,269438,280203,269685,259938,227127,180719,261906,259737,279811,231054,267360,221856,196539,253244,255483,237008,225106,243614,220835,188124,251722,265759,257138,245895,227903,221851,203165,230315,260575,253768,230199,268443,223358,181031,240072,244036,237853,225369,247207,211134,184145,231012,131875,209321,213017,235792,239771,199627,249002,275574,260955,257965,284001,260521,263310,255362,311002,274866,347319,312250,266832,210852,236772,196283,315734,203805,208169,226485,261891,223771,281116,271466,280724,283670,236811,195289,269280,277948,276225,256374,279488,217890,193240,253492,256230,263320,268068,292079,228257,197683,218204,282917,235723,255257,227615,203861,185277,226603,263648,257961,275976,254438,212678,203653,295002,282132,232826,273243,283699,231226,238385,296946,303101,292459,293164,309819,293140,283473,314725,334438,329579,356985,312409,312257,289131,316096,347220],[4488916,3918072,3892124,4099704,3425069,4359179,3347227,4359179,4333232,4670549,4021862,4119773,3757901,4286791,4398136,4565154,4732172,3674392,4314627,3952755,4565154,4592990,4091937,4064100,4976738,4562010,4828621,4532386,4976738,4887867,4473139,5302596,4858244,5035985,4325022,5687700,5432357,5533581,5094944,5567322,5567322,5094944,4757530,4926237,4959978,4757530,4723789,5162426,4993720,5129906,5129906,5342178,4882255,4953012,4917634,5660586,5165284,4316196,5023770,4634604,5554450,4245439,5448314,5802100,6622799,5988701,5389831,5248920,5002327,5213693,5812563,4861416,6939848,6094384,6235295,5530742,5559050,5079132,6518886,5918988,6598872,5958981,6478893,6158947,6398906,6878824,7118783,5918988,5878995,6585327,6414280,6713613,6285994,6200471,6371518,6499804,5901138,7654374,7568850,5986661,6542566,7012946,6333395,6149818,6608760,6195712,6792336,5966241,6975913,5966241,8215055,6838231,6930019,5966241,5277829,4084581,7297172,4459278,4459278,4635883,6843446,5695513,6578538,7417412,7196656,7682320,7240807,6799294,7726471,8697799,7430761,8072925,8027056,7751843,6329907,7981187,8623352,8715090,7981187,8256401,7751843,8072925,6965995,8556656,6417492,9653663,7624200,9105159,7953302,6856295,7569349,8063003,9708513,9269710,7569349,7130547,8380132,10223761,7486251,6983443,8882940,8827072,10167894,8994675,8938808,7765589,8547735,7597986,7877324,11601113,10351762,9756834,8983426,10589734,10649227,9102412,10827706,10173284,9935312],[3119179,2720216,2878278,3149553,2760373,3500746,2779170,3610789,3505620,4062173,3715394,3665041,3468077,3731718,3885821,4018028,4090703,3356332,4241264,3986175,4746880,4862317,4331490,4499105,5668946,5524488,5871334,5119620,5121737,5262057,5260814,6132064,5808558,6245229,5437775,7190853,5967105,6586584,6705146,7060708,6905690,6408509,5916953,6255384,5779114,5967684,5217093,5540766,5304594,5461151,5066180,5761788,5639440,5729388,5791396,6125532,5333680,4323782,5067818,4893545,5859694,4529058,5463199,5415823,6165437,5975339,5536125,5284211,5092047,5027244,5589288,4618491,6117733,5418690,5595773,4943868,4133223,3915180,4114452,4307538,5135591,4503487,6303560,5613835,5890365,5998935,6381473,4978580,4829128,5348203,5563273,5536230,5226926,5550845,5102353,5295684,5009853,6553817,7255240,5802572,6700312,7395324,6117049,6163928,7130611,7067967,8147957,7782196,9146734,8320553,10849951,8775120,8768358,7392849,6787387,5749752,10230715,6929316,7153228,7319122,9642616,8451776,10384095,11046351,11260487,12131998,10374578,9700710,10527922,11928774,10290919,11050159,10028960,9554034,7845878,9592187,10193192,9617967,7489085,8227898,8161423,8368700,6960685,8091884,6147322,8287404,6520223,7983282,6815446,6522957,7065925,8033184,9528010,9047637,7690943,7369622,8729899,10134350,7436308,7204664,8954116,8787393,10645076,9983564,10009312,8773874,10754101,10610179,11022592,14217107,13050105,12204887,11106218,13368297,13875337,12037733,14108349,13224118,12165126],[4488916,3918072,3892124,4099704,3425069,4359179,3347227,4359179,4333232,4670549,4021862,4119773,3757901,4286791,4398136,4565154,4732172,3674392,4314627,3952755,4565154,4592990,4091937,4064100,4976738,4562010,4828621,4532386,4976738,4887867,4473139,5302596,4858244,5035985,4325022,5687700,5432357,5533581,5094944,5567322,5567322,5094944,4757530,4926237,4959978,4757530,4723789,5162426,4993720,5129906,5129906,5342178,4882255,4953012,4917634,5660586,5165284,4316196,5023770,4634604,5554450,4245439,5448314,5802100,6622799,5988701,5389831,5248920,5002327,5213693,5812563,4861416,6939848,6094384,6235295,5530742,5559050,5079132,6518886,5918988,6598872,5958981,6478893,6158947,6398906,6878824,7118783,5918988,5878995,6585327,6414280,6713613,6285994,6200471,6371518,6499804,5901138,7654374,7568850,5986661,6542566,7012946,6333395,6149818,6608760,6195712,6792336,5966241,6975913,5966241,8215055,6838231,6930019,5966241,5277829,4084581,7297172,4459278,4459278,4635883,6843446,5695513,6578538,7417412,7196656,7682320,7240807,6799294,7726471,8697799,7430761,8072925,8027056,7751843,6329907,7981187,8623352,8715090,7981187,8256401,7751843,8072925,6965995,8556656,6417492,9653663,7624200,9105159,7953302,6856295,7569349,8063003,9708513,9269710,7569349,7130547,8380132,10223761,7486251,6983443,8882940,8827072,10167894,8994675,8938808,7765589,8547735,7597986,7877324,11601113,10351762,9756834,8983426,10589734,10649227,9102412,10827706,10173284,9935312],[9,10,10,10,10,10,11,10,11,12,12,12,12,12,12,11,13,15,14,14,15,14,14,14,16,17,17,16,16,18,17,18,19,20,21,24,23,24,23,21,23,23,24,29,22,21,19,20,20,24,25,22,22,21,22,24,24,25,20,19,22,20,21,25,32,24,22,23,23,21,26,25,25,21,23,21,19,18,21,19,20,18,28,22,27,34,27,21,21,25,23,27,29,25,40,26,24,29,31,30,28,28,24,25,26,28,32,31,31,31,32,29,34,36,30,30,33,35,35,33,38,39,38,42,41,44,45,52,41,44,39,45,37,45,42,39,41,38,29,29,37,44,33,30,27,33,30,40,38,30,28,32,36,37,37,37,31,37,33,27,33,39,46,35,34,31,38,36,39,52,43,38,35,39,46,40,50,43,36],[3119179,2720216,2878278,3149553,2760373,3500746,2779170,3610789,3505620,4062173,3715394,3665041,3468077,3731718,3885821,4018028,4090703,3356332,4241264,3986175,4746880,4862317,4331490,4499105,5668946,5524488,5871334,5119620,5121737,5262057,5260814,6132064,5808558,6245229,5437775,7190853,5967105,6586584,6705146,7060708,6905690,6408509,5916953,6255384,5779114,5967684,5217093,5540766,5304594,5461151,5066180,5761788,5639440,5729388,5791396,6125532,5333680,4323782,5067818,4893545,5859694,4529058,5463199,5415823,6165437,5975339,5536125,5284211,5092047,5027244,5589288,4618491,6117733,5418690,5595773,4943868,4133223,3915180,4114452,4307538,5135591,4503487,6303560,5613835,5890365,5998935,6381473,4978580,4829128,5348203,5563273,5536230,5226926,5550845,5102353,5295684,5009853,6553817,7255240,5802572,6700312,7395324,6117049,6163928,7130611,7067967,8147957,7782196,9146734,8320553,10849951,8775120,8768358,7392849,6787387,5749752,10230715,6929316,7153228,7319122,9642616,8451776,10384095,11046351,11260487,12131998,10374578,9700710,10527922,11928774,10290919,11050159,10028960,9554034,7845878,9592187,10193192,9617967,7489085,8227898,8161423,8368700,6960685,8091884,6147322,8287404,6520223,7983282,6815446,6522957,7065925,8033184,9528010,9047637,7690943,7369622,8729899,10134350,7436308,7204664,8954116,8787393,10645076,9983564,10009312,8773874,10754101,10610179,11022592,14217107,13050105,12204887,11106218,13368297,13875337,12037733,14108349,13224118,12165126],[256,199,228,273,247,307,261,297,277,316,303,303,280,322,301,297,327,343,486,392,443,388,353,407,468,473,499,470,529,421,495,588,570,569,470,546,442,475,511,668,491,456,391,356,410,479,455,435,375,311,278,387,388,463,447,339,307,259,375,368,386,341,357,255,198,283,294,226,242,241,192,157,206,208,222,211,215,378,167,198,225,202,229,250,171,125,167,168,168,166,188,156,138,195,76,131,124,113,114,88,122,159,158,193,235,211,190,249,350,348,365,338,266,209,298,334,542,386,426,425,359,378,462,388,413,383,290,239,325,282,273,257,218,145,143,160,149,134,154,13,120,99,168,174,146,136,117,100,84,147,150,143,134,115,101,119,180,153,137,160,149,108,112,149,167,148,174,282,228,163,193,209,192,192,162,168,155,186,200]]},\"header\":{\"align\":\"left\",\"font\":{\"size\":10},\"values\":[\"Date\",\"Number\\u003cbr\\u003eTransactions\",\"Output\\u003cbr\\u003eVolume (BTC)\",\"Market\\u003cbr\\u003ePrice\",\"Hash\\u003cbr\\u003eRate\",\"Cost per\\u003cbr\\u003etrans-USD\",\"Mining\\u003cbr\\u003eRevenue-USD\",\"Trasaction\\u003cbr\\u003efees-BTC\"]},\"type\":\"table\",\"domain\":{\"x\":[0.0,1.0],\"y\":[0.6866666666666665,0.9999999999999998]}}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"matches\":\"x2\",\"showticklabels\":false},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.34333333333333327,0.6566666666666665]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,0.3133333333333333]},\"title\":{\"text\":\"Bitcoin mining stats for 180 days\"},\"height\":800,\"showlegend\":false}}"},{"location":"examples/agent/react_agent/","title":"ReAct Agent - A Simple Intro with Calculator Tools","text":"<p>If you're opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-llms-openai\n</pre> %pip install llama-index-llms-openai In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index\n</pre> !pip install llama-index In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.agent import ReActAgent\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.core.tools import BaseTool, FunctionTool\n</pre> from llama_index.core.agent import ReActAgent from llama_index.llms.openai import OpenAI from llama_index.core.llms import ChatMessage from llama_index.core.tools import BaseTool, FunctionTool <pre>[nltk_data] Downloading package stopwords to /Users/jerryliu/Programmi\n[nltk_data]     ng/gpt_index/.venv/lib/python3.10/site-\n[nltk_data]     packages/llama_index/legacy/_static/nltk_cache...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to /Users/jerryliu/Programming/g\n[nltk_data]     pt_index/.venv/lib/python3.10/site-\n[nltk_data]     packages/llama_index/legacy/_static/nltk_cache...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n</pre> In\u00a0[\u00a0]: Copied! <pre>def multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two integers and returns the result integer\"\"\"\n    return a * b\n\n\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\n</pre> def multiply(a: int, b: int) -&gt; int:     \"\"\"Multiply two integers and returns the result integer\"\"\"     return a * b   multiply_tool = FunctionTool.from_defaults(fn=multiply) In\u00a0[\u00a0]: Copied! <pre>def add(a: int, b: int) -&gt; int:\n    \"\"\"Add two integers and returns the result integer\"\"\"\n    return a + b\n\n\nadd_tool = FunctionTool.from_defaults(fn=add)\n</pre> def add(a: int, b: int) -&gt; int:     \"\"\"Add two integers and returns the result integer\"\"\"     return a + b   add_tool = FunctionTool.from_defaults(fn=add) In\u00a0[\u00a0]: Copied! <pre>llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\nagent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\n</pre> llm = OpenAI(model=\"gpt-3.5-turbo-instruct\") agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True) In\u00a0[\u00a0]: Copied! <pre>response = agent.chat(\"What is 20+(2*4)? Calculate step by step \")\n</pre> response = agent.chat(\"What is 20+(2*4)? Calculate step by step \") <pre>Thought: I need to use a tool to help me answer the question.\nAction: multiply\nAction Input: {\"a\": 2, \"b\": 4}\nObservation: 8\nThought: I need to use a tool to help me answer the question.\nAction: add\nAction Input: {\"a\": 20, \"b\": 8}\nObservation: 28\nThought: I can answer without using any more tools.\nAnswer: 28\n</pre> In\u00a0[\u00a0]: Copied! <pre>response_gen = agent.stream_chat(\"What is 20+2*4? Calculate step by step\")\nresponse_gen.print_response_stream()\n</pre> response_gen = agent.stream_chat(\"What is 20+2*4? Calculate step by step\") response_gen.print_response_stream() <pre>28</pre> In\u00a0[\u00a0]: Copied! <pre>llm = OpenAI(model=\"gpt-4\")\nagent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\n</pre> llm = OpenAI(model=\"gpt-4\") agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True) In\u00a0[\u00a0]: Copied! <pre>response = agent.chat(\"What is 2+2*4\")\nprint(response)\n</pre> response = agent.chat(\"What is 2+2*4\") print(response) <pre>Thought: I need to use the tools to help me answer the question. According to the order of operations in mathematics (BIDMAS/BODMAS), multiplication should be done before addition. So, I will first multiply 2 and 4, and then add the result to 2.\nAction: multiply\nAction Input: {'a': 2, 'b': 4}\nObservation: 8\nThought: Now that I have the result of the multiplication, I need to add this result to 2.\nAction: add\nAction Input: {'a': 2, 'b': 8}\nObservation: 10\nThought: I can answer without using any more tools.\nAnswer: 10\n10\n</pre> In\u00a0[\u00a0]: Copied! <pre>llm = OpenAI(model=\"gpt-4\")\nagent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\n</pre> llm = OpenAI(model=\"gpt-4\") agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True) In\u00a0[\u00a0]: Copied! <pre>prompt_dict = agent.get_prompts()\nfor k, v in prompt_dict.items():\n    print(f\"Prompt: {k}\\n\\nValue: {v.template}\")\n</pre> prompt_dict = agent.get_prompts() for k, v in prompt_dict.items():     print(f\"Prompt: {k}\\n\\nValue: {v.template}\") <pre>Prompt: agent_worker:system_prompt\n\nValue: \nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\n\n## Tools\nYou have access to a wide variety of tools. You are responsible for using\nthe tools in any sequence you deem appropriate to complete the task at hand.\nThis may require breaking the task into subtasks and using different tools\nto complete each subtask.\n\nYou have access to the following tools:\n{tool_desc}\n\n## Output Format\nTo answer the question, please use the following format.\n\n```\nThought: I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names}) if using a tool.\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n```\n\nPlease ALWAYS start with a Thought.\n\nPlease use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n\nIf this format is used, the user will respond in the following format:\n\n```\nObservation: tool response\n```\n\nYou should keep repeating the above format until you have enough information\nto answer the question without using any more tools. At that point, you MUST respond\nin the one of the following two formats:\n\n```\nThought: I can answer without using any more tools.\nAnswer: [your answer here]\n```\n\n```\nThought: I cannot answer the question with the provided tools.\nAnswer: Sorry, I cannot answer your query.\n```\n\n## Current Conversation\nBelow is the current conversation consisting of interleaving human and assistant messages.\n\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import PromptTemplate\n\nreact_system_header_str = \"\"\"\\\n\nYou are designed to help with a variety of tasks, from answering questions \\\n    to providing summaries to other types of analyses.\n\n## Tools\nYou have access to a wide variety of tools. You are responsible for using\nthe tools in any sequence you deem appropriate to complete the task at hand.\nThis may require breaking the task into subtasks and using different tools\nto complete each subtask.\n\nYou have access to the following tools:\n{tool_desc}\n\n## Output Format\nTo answer the question, please use the following format.\n\n```\nThought: I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names}) if using a tool.\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n```\n\nPlease ALWAYS start with a Thought.\n\nPlease use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n\nIf this format is used, the user will respond in the following format:\n\n```\nObservation: tool response\n```\n\nYou should keep repeating the above format until you have enough information\nto answer the question without using any more tools. At that point, you MUST respond\nin the one of the following two formats:\n\n```\nThought: I can answer without using any more tools.\nAnswer: [your answer here]\n```\n\n```\nThought: I cannot answer the question with the provided tools.\nAnswer: Sorry, I cannot answer your query.\n```\n\n## Additional Rules\n- The answer MUST contain a sequence of bullet points that explain how you arrived at the answer. This can include aspects of the previous conversation history.\n- You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n\n## Current Conversation\nBelow is the current conversation consisting of interleaving human and assistant messages.\n\n\"\"\"\nreact_system_prompt = PromptTemplate(react_system_header_str)\n</pre> from llama_index.core import PromptTemplate  react_system_header_str = \"\"\"\\  You are designed to help with a variety of tasks, from answering questions \\     to providing summaries to other types of analyses.  ## Tools You have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand. This may require breaking the task into subtasks and using different tools to complete each subtask.  You have access to the following tools: {tool_desc}  ## Output Format To answer the question, please use the following format.  ``` Thought: I need to use a tool to help me answer the question. Action: tool name (one of {tool_names}) if using a tool. Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}}) ```  Please ALWAYS start with a Thought.  Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.  If this format is used, the user will respond in the following format:  ``` Observation: tool response ```  You should keep repeating the above format until you have enough information to answer the question without using any more tools. At that point, you MUST respond in the one of the following two formats:  ``` Thought: I can answer without using any more tools. Answer: [your answer here] ```  ``` Thought: I cannot answer the question with the provided tools. Answer: Sorry, I cannot answer your query. ```  ## Additional Rules - The answer MUST contain a sequence of bullet points that explain how you arrived at the answer. This can include aspects of the previous conversation history. - You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.  ## Current Conversation Below is the current conversation consisting of interleaving human and assistant messages.  \"\"\" react_system_prompt = PromptTemplate(react_system_header_str) In\u00a0[\u00a0]: Copied! <pre>agent.get_prompts()\n</pre> agent.get_prompts() Out[\u00a0]: <pre>{'agent_worker:system_prompt': PromptTemplate(metadata={'prompt_type': &lt;PromptType.CUSTOM: 'custom'&gt;}, template_vars=['tool_desc', 'tool_names'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Tools\\nYou have access to a wide variety of tools. You are responsible for using\\nthe tools in any sequence you deem appropriate to complete the task at hand.\\nThis may require breaking the task into subtasks and using different tools\\nto complete each subtask.\\n\\nYou have access to the following tools:\\n{tool_desc}\\n\\n## Output Format\\nTo answer the question, please use the following format.\\n\\n```\\nThought: I need to use a tool to help me answer the question.\\nAction: tool name (one of {tool_names}) if using a tool.\\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\\n```\\n\\nPlease ALWAYS start with a Thought.\\n\\nPlease use a valid JSON format for the Action Input. Do NOT do this {{\\'input\\': \\'hello world\\', \\'num_beams\\': 5}}.\\n\\nIf this format is used, the user will respond in the following format:\\n\\n```\\nObservation: tool response\\n```\\n\\nYou should keep repeating the above format until you have enough information\\nto answer the question without using any more tools. At that point, you MUST respond\\nin the one of the following two formats:\\n\\n```\\nThought: I can answer without using any more tools.\\nAnswer: [your answer here]\\n```\\n\\n```\\nThought: I cannot answer the question with the provided tools.\\nAnswer: Sorry, I cannot answer your query.\\n```\\n\\n## Current Conversation\\nBelow is the current conversation consisting of interleaving human and assistant messages.\\n\\n')}</pre> In\u00a0[\u00a0]: Copied! <pre>agent.update_prompts({\"agent_worker:system_prompt\": react_system_prompt})\n</pre> agent.update_prompts({\"agent_worker:system_prompt\": react_system_prompt}) In\u00a0[\u00a0]: Copied! <pre>agent.reset()\nresponse = agent.chat(\"What is 5+3+2\")\nprint(response)\n</pre> agent.reset() response = agent.chat(\"What is 5+3+2\") print(response) <pre>Thought: I need to use the add tool to help me answer the question.\nAction: add\nAction Input: {'a': 5, 'b': 3}\nObservation: 8\nThought: Now I need to add the result from the previous operation to 2.\nAction: add\nAction Input: {'a': 8, 'b': 2}\nObservation: 10\nThought: I can answer without using any more tools.\nAnswer: The result of 5+3+2 is 10.\n\n- I first added 5 and 3 using the add tool, which resulted in 8.\n- Then I added the result (8) to 2 using the add tool again, which resulted in 10.\nThe result of 5+3+2 is 10.\n\n- I first added 5 and 3 using the add tool, which resulted in 8.\n- Then I added the result (8) to 2 using the add tool again, which resulted in 10.\n</pre>"},{"location":"examples/agent/react_agent/#react-agent-a-simple-intro-with-calculator-tools","title":"ReAct Agent - A Simple Intro with Calculator Tools\u00b6","text":"<p>This is a notebook that showcases the ReAct agent over very simple calculator tools (no fancy RAG pipelines or API calls).</p> <p>We show how it can reason step-by-step over different tools to achieve the end goal.</p>"},{"location":"examples/agent/react_agent/#define-function-tools","title":"Define Function Tools\u00b6","text":"<p>We setup some trivial <code>multiply</code> and <code>add</code> tools. Note that you can define arbitrary functions and pass it to the <code>FunctionTool</code> (which will process the docstring and parameter signature).</p>"},{"location":"examples/agent/react_agent/#run-some-queries","title":"Run Some Queries\u00b6","text":""},{"location":"examples/agent/react_agent/#gpt-35-turbo","title":"gpt-3.5-turbo\u00b6","text":""},{"location":"examples/agent/react_agent/#gpt-4","title":"gpt-4\u00b6","text":""},{"location":"examples/agent/react_agent/#view-prompts","title":"View Prompts\u00b6","text":"<p>Let's take a look at the core system prompt powering the ReAct agent!</p> <p>Within the agent, the current conversation history is dumped below this line.</p>"},{"location":"examples/agent/react_agent/#customizing-the-prompt","title":"Customizing the Prompt\u00b6","text":"<p>For fun, let's try instructing the agent to output the answer along with reasoning in bullet points. See \"## Additional Rules\" section.</p>"},{"location":"examples/agent/react_agent_with_query_engine/","title":"ReAct Agent with Query Engine (RAG) Tools","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-llms-openai\n</pre> %pip install llama-index-llms-openai In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    StorageContext,\n    load_index_from_storage,\n)\n\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n</pre> from llama_index.core import (     SimpleDirectoryReader,     VectorStoreIndex,     StorageContext,     load_index_from_storage, )  from llama_index.core.tools import QueryEngineTool, ToolMetadata In\u00a0[\u00a0]: Copied! <pre>try:\n    storage_context = StorageContext.from_defaults(\n        persist_dir=\"./storage/lyft\"\n    )\n    lyft_index = load_index_from_storage(storage_context)\n\n    storage_context = StorageContext.from_defaults(\n        persist_dir=\"./storage/uber\"\n    )\n    uber_index = load_index_from_storage(storage_context)\n\n    index_loaded = True\nexcept:\n    index_loaded = False\n</pre> try:     storage_context = StorageContext.from_defaults(         persist_dir=\"./storage/lyft\"     )     lyft_index = load_index_from_storage(storage_context)      storage_context = StorageContext.from_defaults(         persist_dir=\"./storage/uber\"     )     uber_index = load_index_from_storage(storage_context)      index_loaded = True except:     index_loaded = False <p>Download Data</p> In\u00a0[\u00a0]: Copied! <pre>!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'\n</pre> !mkdir -p 'data/10k/' !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf' !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf' In\u00a0[\u00a0]: Copied! <pre>if not index_loaded:\n    # load data\n    lyft_docs = SimpleDirectoryReader(\n        input_files=[\"./data/10k/lyft_2021.pdf\"]\n    ).load_data()\n    uber_docs = SimpleDirectoryReader(\n        input_files=[\"./data/10k/uber_2021.pdf\"]\n    ).load_data()\n\n    # build index\n    lyft_index = VectorStoreIndex.from_documents(lyft_docs)\n    uber_index = VectorStoreIndex.from_documents(uber_docs)\n\n    # persist index\n    lyft_index.storage_context.persist(persist_dir=\"./storage/lyft\")\n    uber_index.storage_context.persist(persist_dir=\"./storage/uber\")\n</pre> if not index_loaded:     # load data     lyft_docs = SimpleDirectoryReader(         input_files=[\"./data/10k/lyft_2021.pdf\"]     ).load_data()     uber_docs = SimpleDirectoryReader(         input_files=[\"./data/10k/uber_2021.pdf\"]     ).load_data()      # build index     lyft_index = VectorStoreIndex.from_documents(lyft_docs)     uber_index = VectorStoreIndex.from_documents(uber_docs)      # persist index     lyft_index.storage_context.persist(persist_dir=\"./storage/lyft\")     uber_index.storage_context.persist(persist_dir=\"./storage/uber\") In\u00a0[\u00a0]: Copied! <pre>lyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\nuber_engine = uber_index.as_query_engine(similarity_top_k=3)\n</pre> lyft_engine = lyft_index.as_query_engine(similarity_top_k=3) uber_engine = uber_index.as_query_engine(similarity_top_k=3) In\u00a0[\u00a0]: Copied! <pre>query_engine_tools = [\n    QueryEngineTool(\n        query_engine=lyft_engine,\n        metadata=ToolMetadata(\n            name=\"lyft_10k\",\n            description=(\n                \"Provides information about Lyft financials for year 2021. \"\n                \"Use a detailed plain text question as input to the tool.\"\n            ),\n        ),\n    ),\n    QueryEngineTool(\n        query_engine=uber_engine,\n        metadata=ToolMetadata(\n            name=\"uber_10k\",\n            description=(\n                \"Provides information about Uber financials for year 2021. \"\n                \"Use a detailed plain text question as input to the tool.\"\n            ),\n        ),\n    ),\n]\n</pre> query_engine_tools = [     QueryEngineTool(         query_engine=lyft_engine,         metadata=ToolMetadata(             name=\"lyft_10k\",             description=(                 \"Provides information about Lyft financials for year 2021. \"                 \"Use a detailed plain text question as input to the tool.\"             ),         ),     ),     QueryEngineTool(         query_engine=uber_engine,         metadata=ToolMetadata(             name=\"uber_10k\",             description=(                 \"Provides information about Uber financials for year 2021. \"                 \"Use a detailed plain text question as input to the tool.\"             ),         ),     ), ] In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.agent import ReActAgent\nfrom llama_index.llms.openai import OpenAI\n</pre> from llama_index.core.agent import ReActAgent from llama_index.llms.openai import OpenAI In\u00a0[\u00a0]: Copied! <pre># [Optional] Add Context\n# context = \"\"\"\\\n# You are a stock market sorcerer who is an expert on the companies Lyft and Uber.\\\n#     You will answer questions about Uber and Lyft as in the persona of a sorcerer \\\n#     and veteran stock market investor.\n# \"\"\"\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\nagent = ReActAgent.from_tools(\n    query_engine_tools,\n    llm=llm,\n    verbose=True,\n    # context=context\n)\n</pre> # [Optional] Add Context # context = \"\"\"\\ # You are a stock market sorcerer who is an expert on the companies Lyft and Uber.\\ #     You will answer questions about Uber and Lyft as in the persona of a sorcerer \\ #     and veteran stock market investor. # \"\"\" llm = OpenAI(model=\"gpt-3.5-turbo-0613\")  agent = ReActAgent.from_tools(     query_engine_tools,     llm=llm,     verbose=True,     # context=context ) In\u00a0[\u00a0]: Copied! <pre>response = agent.chat(\"What was Lyft's revenue growth in 2021?\")\nprint(str(response))\n</pre> response = agent.chat(\"What was Lyft's revenue growth in 2021?\") print(str(response)) <pre>Thought: I need to use a tool to help me answer the question.\nAction: lyft_10k\nAction Input: {'input': \"What was Lyft's revenue growth in 2021?\"}\nObservation: Lyft's revenue growth in 2021 was 36%.\nResponse: Lyft's revenue growth in 2021 was 36%.\nLyft's revenue growth in 2021 was 36%.\n</pre> In\u00a0[\u00a0]: Copied! <pre>response = agent.chat(\n    \"Compare and contrast the revenue growth of Uber and Lyft in 2021, then\"\n    \" give an analysis\"\n)\nprint(str(response))\n</pre> response = agent.chat(     \"Compare and contrast the revenue growth of Uber and Lyft in 2021, then\"     \" give an analysis\" ) print(str(response)) <pre>Thought: I need to use a tool to help me compare the revenue growth of Uber and Lyft in 2021.\nAction: lyft_10k\nAction Input: {'input': \"What was Lyft's revenue growth in 2021?\"}\nObservation: Lyft's revenue growth in 2021 was 36%.\nThought: I need to use a tool to help me compare the revenue growth of Uber and Lyft in 2021.\nAction: uber_10k\nAction Input: {'input': \"What was Uber's revenue growth in 2021?\"}\nObservation: Uber's revenue growth in 2021 was 57%.\nResponse: In 2021, Lyft's revenue growth was 36% while Uber's revenue growth was 57%. This indicates that Uber experienced a higher revenue growth compared to Lyft in 2021.\nIn 2021, Lyft's revenue growth was 36% while Uber's revenue growth was 57%. This indicates that Uber experienced a higher revenue growth compared to Lyft in 2021.\n</pre> <p>Async execution: Here we try another query with async execution</p> In\u00a0[\u00a0]: Copied! <pre># Try another query with async execution\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nresponse = await agent.achat(\n    \"Compare and contrast the risks of Uber and Lyft in 2021, then give an\"\n    \" analysis\"\n)\nprint(str(response))\n</pre> # Try another query with async execution  import nest_asyncio  nest_asyncio.apply()  response = await agent.achat(     \"Compare and contrast the risks of Uber and Lyft in 2021, then give an\"     \" analysis\" ) print(str(response)) In\u00a0[\u00a0]: Copied! <pre>llm_instruct = OpenAI(model=\"gpt-3.5-turbo-instruct\")\nagent_instruct = ReActAgent.from_tools(\n    query_engine_tools, llm=llm_instruct, verbose=True\n)\n</pre> llm_instruct = OpenAI(model=\"gpt-3.5-turbo-instruct\") agent_instruct = ReActAgent.from_tools(     query_engine_tools, llm=llm_instruct, verbose=True ) In\u00a0[\u00a0]: Copied! <pre>response = agent_instruct.chat(\"What was Lyft's revenue growth in 2021?\")\nprint(str(response))\n</pre> response = agent_instruct.chat(\"What was Lyft's revenue growth in 2021?\") print(str(response)) <pre>Thought: I need to use a tool to help me answer the question.\nAction: lyft_10k\nAction Input: {'input': \"What was Lyft's revenue growth in 2021?\"}\nObservation: Lyft's revenue growth in 2021 was 36%.\nResponse: Lyft's revenue growth in 2021 was 36%.\nLyft's revenue growth in 2021 was 36%.\n</pre> In\u00a0[\u00a0]: Copied! <pre>response = agent.chat(\n    \"Compare and contrast the revenue growth of Uber and Lyft in 2021, then\"\n    \" give an analysis\"\n)\nprint(str(response))\n</pre> response = agent.chat(     \"Compare and contrast the revenue growth of Uber and Lyft in 2021, then\"     \" give an analysis\" ) print(str(response)) <pre>Thought: I need to use a tool to help me compare the revenue growth of Uber and Lyft in 2021.\nAction: uber_10k\nAction Input: {'input': \"Please provide information about Uber's revenue growth in 2021.\"}\nObservation: Uber's revenue grew by 57% in 2021 compared to the previous year. This growth was primarily driven by an increase in Gross Bookings, with Delivery Gross Bookings increasing by 71% and Mobility Gross Bookings growing by 38%. The increase in Delivery Gross Bookings was due to higher demand for food delivery orders and expansion across U.S. and international markets. The growth in Mobility Gross Bookings was a result of increased Trip volumes as the business recovered from the impacts of COVID-19.\nThought: I have information about Uber's revenue growth in 2021. Now I need to use a tool to get information about Lyft's revenue growth in 2021.\nAction: lyft_10k\nAction Input: {'input': \"Please provide information about Lyft's revenue growth in 2021.\"}\nObservation: Lyft's revenue increased by 36% in 2021 compared to the prior year.\nResponse: In 2021, Uber experienced a higher revenue growth rate of 57% compared to Lyft's growth rate of 36%. This indicates that Uber had a stronger performance in terms of revenue growth during that period. The growth in Uber's revenue was primarily driven by an increase in Gross Bookings, with both Delivery and Mobility segments contributing to the growth. The increase in Delivery Gross Bookings was due to higher demand for food delivery services, while the growth in Mobility Gross Bookings was a result of increased trip volumes as the business recovered from the impacts of COVID-19.\nIn 2021, Uber experienced a higher revenue growth rate of 57% compared to Lyft's growth rate of 36%. This indicates that Uber had a stronger performance in terms of revenue growth during that period. The growth in Uber's revenue was primarily driven by an increase in Gross Bookings, with both Delivery and Mobility segments contributing to the growth. The increase in Delivery Gross Bookings was due to higher demand for food delivery services, while the growth in Mobility Gross Bookings was a result of increased trip volumes as the business recovered from the impacts of COVID-19.\n</pre> In\u00a0[\u00a0]: Copied! <pre>response = agent_instruct.chat(\n    \"Compare and contrast the revenue growth of Uber and Lyft in 2021, then\"\n    \" give an analysis\"\n)\nprint(str(response))\n</pre> response = agent_instruct.chat(     \"Compare and contrast the revenue growth of Uber and Lyft in 2021, then\"     \" give an analysis\" ) print(str(response)) <pre>Response: The revenue growth of Uber was higher than Lyft in 2021, with Uber experiencing a 74% growth compared to Lyft's 48%. This indicates that Uber may have had a stronger financial performance in 2021. However, further analysis is needed to fully understand the factors contributing to this difference.\nThe revenue growth of Uber was higher than Lyft in 2021, with Uber experiencing a 74% growth compared to Lyft's 48%. This indicates that Uber may have had a stronger financial performance in 2021. However, further analysis is needed to fully understand the factors contributing to this difference.\n</pre> In\u00a0[\u00a0]: Copied! <pre>response = agent.chat(\n    \"Can you tell me about the risk factors of the company with the higher\"\n    \" revenue?\"\n)\nprint(str(response))\n</pre> response = agent.chat(     \"Can you tell me about the risk factors of the company with the higher\"     \" revenue?\" ) print(str(response)) <pre>Thought: I need to find out which company has higher revenue before I can provide information about its risk factors.\nAction: lyft_10k\nAction Input: {'input': 'What is the revenue of Lyft in 2021?'}\nObservation: The revenue of Lyft in 2021 is $3,208,323,000.\nThought: Now that I know Lyft has higher revenue, I can find information about its risk factors.\nAction: lyft_10k\nAction Input: {'input': 'What are the risk factors of Lyft?'}\nObservation: Lyft faces numerous risk factors that could potentially harm its business, financial condition, and results of operations. These risk factors include general economic factors such as the impact of the COVID-19 pandemic, natural disasters, economic downturns, and political crises. Operational factors such as limited operating history, financial performance, competition, unpredictability of results, uncertainty regarding market growth, ability to attract and retain drivers and riders, insurance coverage, autonomous vehicle technology, reputation and brand, illegal or improper activity on the platform, accuracy of background checks, changes to pricing practices, growth management, security and privacy breaches, reliance on third parties, and ability to operate various programs and services. Additionally, Lyft faces risks related to its evolving business, including forecasting revenue and managing expenses, complying with laws and regulations, managing assets and expenses during the COVID-19 pandemic, capital expenditures, asset development and utilization, macroeconomic changes, reputation and brand management, growth and business operations, geographic expansion, talent acquisition and retention, platform development, and real estate portfolio management. Furthermore, Lyft's financial performance in recent periods may not be indicative of future performance, and achieving or maintaining profitability in the future is not guaranteed. The Express Drive program and Lyft Rentals program also expose Lyft to risks related to vehicle rental partners, residual value of vehicles, and payment processing.\nResponse: Lyft faces numerous risk factors that could potentially harm its business, financial condition, and results of operations. These risk factors include general economic factors such as the impact of the COVID-19 pandemic, natural disasters, economic downturns, and political crises. Operational factors such as limited operating history, financial performance, competition, unpredictability of results, uncertainty regarding market growth, ability to attract and retain drivers and riders, insurance coverage, autonomous vehicle technology, reputation and brand, illegal or improper activity on the platform, accuracy of background checks, changes to pricing practices, growth management, security and privacy breaches, reliance on third parties, and ability to operate various programs and services. Additionally, Lyft faces risks related to its evolving business, including forecasting revenue and managing expenses, complying with laws and regulations, managing assets and expenses during the COVID-19 pandemic, capital expenditures, asset development and utilization, macroeconomic changes, reputation and brand management, growth and business operations, geographic expansion, talent acquisition and retention, platform development, and real estate portfolio management. Furthermore, Lyft's financial performance in recent periods may not be indicative of future performance, and achieving or maintaining profitability in the future is not guaranteed. The Express Drive program and Lyft Rentals program also expose Lyft to risks related to vehicle rental partners, residual value of vehicles, and payment processing.\nLyft faces numerous risk factors that could potentially harm its business, financial condition, and results of operations. These risk factors include general economic factors such as the impact of the COVID-19 pandemic, natural disasters, economic downturns, and political crises. Operational factors such as limited operating history, financial performance, competition, unpredictability of results, uncertainty regarding market growth, ability to attract and retain drivers and riders, insurance coverage, autonomous vehicle technology, reputation and brand, illegal or improper activity on the platform, accuracy of background checks, changes to pricing practices, growth management, security and privacy breaches, reliance on third parties, and ability to operate various programs and services. Additionally, Lyft faces risks related to its evolving business, including forecasting revenue and managing expenses, complying with laws and regulations, managing assets and expenses during the COVID-19 pandemic, capital expenditures, asset development and utilization, macroeconomic changes, reputation and brand management, growth and business operations, geographic expansion, talent acquisition and retention, platform development, and real estate portfolio management. Furthermore, Lyft's financial performance in recent periods may not be indicative of future performance, and achieving or maintaining profitability in the future is not guaranteed. The Express Drive program and Lyft Rentals program also expose Lyft to risks related to vehicle rental partners, residual value of vehicles, and payment processing.\n</pre> In\u00a0[\u00a0]: Copied! <pre>response = agent_instruct.query(\n    \"Can you tell me about the risk factors of the company with the higher\"\n    \" revenue?\"\n)\nprint(str(response))\n</pre> response = agent_instruct.query(     \"Can you tell me about the risk factors of the company with the higher\"     \" revenue?\" ) print(str(response)) <pre>Response: The risk factors for the company with the higher revenue include competition, regulatory changes, and dependence on drivers.\nThe risk factors for the company with the higher revenue include competition, regulatory changes, and dependence on drivers.\n</pre> <p>Observation: The turbo-instruct agent seems to do worse on agent reasoning compared to the regular turbo model. Of course, this is subject to further observation!</p>"},{"location":"examples/agent/react_agent_with_query_engine/#react-agent-with-query-engine-rag-tools","title":"ReAct Agent with Query Engine (RAG) Tools\u00b6","text":"<p>In this section, we show how to setup an agent powered by the ReAct loop for financial analysis.</p> <p>The agent has access to two \"tools\": one to query the 2021 Lyft 10-K and the other to query the 2021 Uber 10-K.</p> <p>We try two different LLMs:</p> <ul> <li>gpt-3.5-turbo</li> <li>gpt-3.5-turbo-instruct</li> </ul> <p>Note that you can plug in any LLM that exposes a text completion endpoint.</p>"},{"location":"examples/agent/react_agent_with_query_engine/#build-query-engine-tools","title":"Build Query Engine Tools\u00b6","text":""},{"location":"examples/agent/react_agent_with_query_engine/#setup-react-agent","title":"Setup ReAct Agent\u00b6","text":"<p>Here we setup two ReAct agents: one powered by standard gpt-3.5-turbo, and the other powered by gpt-3.5-turbo-instruct.</p> <p>You can optionally specify context which will be added to the core ReAct system prompt.</p>"},{"location":"examples/agent/react_agent_with_query_engine/#run-some-example-queries","title":"Run Some Example Queries\u00b6","text":"<p>We run some example queries using the agent, showcasing some of the agent's abilities to do chain-of-thought-reasoning and tool use to synthesize the right answer.</p> <p>We also show queries.</p>"},{"location":"examples/agent/react_agent_with_query_engine/#compare-gpt-35-turbo-vs-gpt-35-turbo-instruct","title":"Compare gpt-3.5-turbo vs. gpt-3.5-turbo-instruct\u00b6","text":"<p>We compare the performance of the two agents in being able to answer some complex queries.</p>"},{"location":"examples/agent/react_agent_with_query_engine/#taking-a-look-at-a-turbo-instruct-agent","title":"Taking a look at a turbo-instruct agent\u00b6","text":""},{"location":"examples/agent/react_agent_with_query_engine/#try-more-complex-queries","title":"Try more complex queries\u00b6","text":"<p>We compare gpt-3.5-turbo with gpt-3.5-turbo-instruct agents on more complex queries.</p>"},{"location":"examples/pipeline/query_pipeline/","title":"An Introduction to LlamaIndex Query Pipelines","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-embeddings-openai\n%pip install llama-index-postprocessor-cohere-rerank\n%pip install llama-index-llms-openai\n</pre> %pip install llama-index-embeddings-openai %pip install llama-index-postprocessor-cohere-rerank %pip install llama-index-llms-openai In\u00a0[\u00a0]: Copied! <pre># setup Arize Phoenix for logging/observability\nimport phoenix as px\n\npx.launch_app()\nimport llama_index.core\n\nllama_index.core.set_global_handler(\"arize_phoenix\")\n</pre> # setup Arize Phoenix for logging/observability import phoenix as px  px.launch_app() import llama_index.core  llama_index.core.set_global_handler(\"arize_phoenix\") <pre>\ud83c\udf0d To view the Phoenix app in your browser, visit http://127.0.0.1:6006/\n\ud83d\udcfa To view the Phoenix app in a notebook, run `px.active_session().view()`\n\ud83d\udcd6 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import Settings\n\nSettings.llm = OpenAI(model=\"gpt-3.5-turbo\")\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n</pre> from llama_index.llms.openai import OpenAI from llama_index.embeddings.openai import OpenAIEmbedding from llama_index.core import Settings  Settings.llm = OpenAI(model=\"gpt-3.5-turbo\") Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\") In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import SimpleDirectoryReader\n\nreader = SimpleDirectoryReader(\"../data/paul_graham\")\n</pre> from llama_index.core import SimpleDirectoryReader  reader = SimpleDirectoryReader(\"../data/paul_graham\") In\u00a0[\u00a0]: Copied! <pre>docs = reader.load_data()\n</pre> docs = reader.load_data() In\u00a0[\u00a0]: Copied! <pre>import os\nfrom llama_index.core import (\n    StorageContext,\n    VectorStoreIndex,\n    load_index_from_storage,\n)\n\nif not os.path.exists(\"storage\"):\n    index = VectorStoreIndex.from_documents(docs)\n    # save index to disk\n    index.set_index_id(\"vector_index\")\n    index.storage_context.persist(\"./storage\")\nelse:\n    # rebuild storage context\n    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n    # load index\n    index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n</pre> import os from llama_index.core import (     StorageContext,     VectorStoreIndex,     load_index_from_storage, )  if not os.path.exists(\"storage\"):     index = VectorStoreIndex.from_documents(docs)     # save index to disk     index.set_index_id(\"vector_index\")     index.storage_context.persist(\"./storage\") else:     # rebuild storage context     storage_context = StorageContext.from_defaults(persist_dir=\"storage\")     # load index     index = load_index_from_storage(storage_context, index_id=\"vector_index\") In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.query_pipeline import QueryPipeline\nfrom llama_index.core import PromptTemplate\n\n# try chaining basic prompts\nprompt_str = \"Please generate related movies to {movie_name}\"\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\np = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)\n</pre> from llama_index.core.query_pipeline import QueryPipeline from llama_index.core import PromptTemplate  # try chaining basic prompts prompt_str = \"Please generate related movies to {movie_name}\" prompt_tmpl = PromptTemplate(prompt_str) llm = OpenAI(model=\"gpt-3.5-turbo\")  p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True) In\u00a0[\u00a0]: Copied! <pre>output = p.run(movie_name=\"The Departed\")\n</pre> output = p.run(movie_name=\"The Departed\") <pre>&gt; Running module 43554ab5-002f-4705-b5f1-f3fb4fd9bd44 with input: \nmovie_name: The Departed\n\n&gt; Running module 4b086131-f82b-4cb9-b929-427a60aca9c8 with input: \nmessages: Please generate related movies to The Departed\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(str(output))\n</pre> print(str(output)) <pre>assistant: 1. Infernal Affairs (2002) - This is the original Hong Kong film that inspired The Departed. It follows a similar storyline of undercover cops infiltrating a criminal organization.\n\n2. Internal Affairs (1990) - This American crime thriller film shares a similar premise to The Departed, focusing on the corrupt practices within the police force.\n\n3. The Town (2010) - Directed by Ben Affleck, this crime drama revolves around a group of bank robbers in Boston and the FBI agent determined to bring them down.\n\n4. American Gangster (2007) - Based on a true story, this crime film explores the rise and fall of a drug lord in Harlem, New York, and the detective who is determined to bring him to justice.\n\n5. Donnie Brasco (1997) - Starring Johnny Depp and Al Pacino, this crime drama tells the story of an undercover FBI agent who infiltrates the mob and forms a close bond with a mafia hitman.\n\n6. The Departed (2006) - Although already mentioned, it is worth noting that The Departed itself is a highly acclaimed crime thriller directed by Martin Scorsese, exploring the intertwining lives of undercover cops and mobsters in Boston.\n\n7. Training Day (2001) - Denzel Washington won an Academy Award for his role in this crime thriller, where he plays a corrupt narcotics detective who takes a rookie cop under his wing for a day of training.\n\n8. Heat (1995) - Directed by Michael Mann, this crime film features an intense cat-and-mouse game between a skilled detective and a professional thief, both equally obsessed with their work.\n\n9. The Godfather (1972) - Francis Ford Coppola's iconic crime saga follows the Corleone family's rise to power in the world of organized crime, with themes of loyalty, betrayal, and family ties.\n\n10. Casino (1995) - Directed by Martin Scorsese, this film delves into the dark underbelly of Las Vegas casinos, exploring the corrupt and violent world of mob-controlled gambling.\n</pre> In\u00a0[\u00a0]: Copied! <pre>from typing import List\nfrom pydantic import BaseModel, Field\nfrom llama_index.core.output_parsers import PydanticOutputParser\n\n\nclass Movie(BaseModel):\n    \"\"\"Object representing a single movie.\"\"\"\n\n    name: str = Field(..., description=\"Name of the movie.\")\n    year: int = Field(..., description=\"Year of the movie.\")\n\n\nclass Movies(BaseModel):\n    \"\"\"Object representing a list of movies.\"\"\"\n\n    movies: List[Movie] = Field(..., description=\"List of movies.\")\n\n\nllm = OpenAI(model=\"gpt-3.5-turbo\")\noutput_parser = PydanticOutputParser(Movies)\njson_prompt_str = \"\"\"\\\nPlease generate related movies to {movie_name}. Output with the following JSON format: \n\"\"\"\njson_prompt_str = output_parser.format(json_prompt_str)\n</pre> from typing import List from pydantic import BaseModel, Field from llama_index.core.output_parsers import PydanticOutputParser   class Movie(BaseModel):     \"\"\"Object representing a single movie.\"\"\"      name: str = Field(..., description=\"Name of the movie.\")     year: int = Field(..., description=\"Year of the movie.\")   class Movies(BaseModel):     \"\"\"Object representing a list of movies.\"\"\"      movies: List[Movie] = Field(..., description=\"List of movies.\")   llm = OpenAI(model=\"gpt-3.5-turbo\") output_parser = PydanticOutputParser(Movies) json_prompt_str = \"\"\"\\ Please generate related movies to {movie_name}. Output with the following JSON format:  \"\"\" json_prompt_str = output_parser.format(json_prompt_str) In\u00a0[\u00a0]: Copied! <pre># add JSON spec to prompt template\njson_prompt_tmpl = PromptTemplate(json_prompt_str)\n\np = QueryPipeline(chain=[json_prompt_tmpl, llm, output_parser], verbose=True)\noutput = p.run(movie_name=\"Toy Story\")\n</pre> # add JSON spec to prompt template json_prompt_tmpl = PromptTemplate(json_prompt_str)  p = QueryPipeline(chain=[json_prompt_tmpl, llm, output_parser], verbose=True) output = p.run(movie_name=\"Toy Story\") <pre>&gt; Running module 2e4093c5-ae62-420a-be91-9c28c057bada with input: \nmovie_name: Toy Story\n\n&gt; Running module 3b41f95c-f54b-41d7-8ef0-8e45b5d7eeb0 with input: \nmessages: Please generate related movies to Toy Story. Output with the following JSON format: \n\n\n\nHere's a JSON schema to follow:\n{\"title\": \"Movies\", \"description\": \"Object representing a list of movies.\", \"typ...\n\n&gt; Running module 27e79a16-72de-4ce2-8b2e-94932c4069c3 with input: \ninput: assistant: {\n  \"movies\": [\n    {\n      \"name\": \"Finding Nemo\",\n      \"year\": 2003\n    },\n    {\n      \"name\": \"Monsters, Inc.\",\n      \"year\": 2001\n    },\n    {\n      \"name\": \"Cars\",\n      \"year\": 2006\n...\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>output\n</pre> output Out[\u00a0]: <pre>Movies(movies=[Movie(name='Finding Nemo', year=2003), Movie(name='Monsters, Inc.', year=2001), Movie(name='Cars', year=2006), Movie(name='The Incredibles', year=2004), Movie(name='Ratatouille', year=2007)])</pre> <p>1. Chain multiple Prompts with Streaming</p> In\u00a0[\u00a0]: Copied! <pre>prompt_str = \"Please generate related movies to {movie_name}\"\nprompt_tmpl = PromptTemplate(prompt_str)\n# let's add some subsequent prompts for fun\nprompt_str2 = \"\"\"\\\nHere's some text:\n\n{text}\n\nCan you rewrite this with a summary of each movie?\n\"\"\"\nprompt_tmpl2 = PromptTemplate(prompt_str2)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\nllm_c = llm.as_query_component(streaming=True)\n\np = QueryPipeline(\n    chain=[prompt_tmpl, llm_c, prompt_tmpl2, llm_c], verbose=True\n)\n# p = QueryPipeline(chain=[prompt_tmpl, llm_c], verbose=True)\n</pre> prompt_str = \"Please generate related movies to {movie_name}\" prompt_tmpl = PromptTemplate(prompt_str) # let's add some subsequent prompts for fun prompt_str2 = \"\"\"\\ Here's some text:  {text}  Can you rewrite this with a summary of each movie? \"\"\" prompt_tmpl2 = PromptTemplate(prompt_str2) llm = OpenAI(model=\"gpt-3.5-turbo\") llm_c = llm.as_query_component(streaming=True)  p = QueryPipeline(     chain=[prompt_tmpl, llm_c, prompt_tmpl2, llm_c], verbose=True ) # p = QueryPipeline(chain=[prompt_tmpl, llm_c], verbose=True) In\u00a0[\u00a0]: Copied! <pre>output = p.run(movie_name=\"The Dark Knight\")\nfor o in output:\n    print(o.delta, end=\"\")\n</pre> output = p.run(movie_name=\"The Dark Knight\") for o in output:     print(o.delta, end=\"\") <pre>&gt; Running module 213af6d4-3450-46af-9087-b80656ae6951 with input: \nmovie_name: The Dark Knight\n\n&gt; Running module 3ff7e987-f5f3-4b36-a3e1-be5a4821d9d9 with input: \nmessages: Please generate related movies to The Dark Knight\n\n&gt; Running module a2841bd3-c833-4427-9a7e-83b19872b064 with input: \ntext: &lt;generator object llm_chat_callback.&lt;locals&gt;.wrap.&lt;locals&gt;.wrapped_llm_chat.&lt;locals&gt;.wrapped_gen at 0x298d338b0&gt;\n\n&gt; Running module c7e0a454-213a-460e-b029-f2d42fd7d938 with input: \nmessages: Here's some text:\n\n1. Batman Begins (2005)\n2. The Dark Knight Rises (2012)\n3. Batman v Superman: Dawn of Justice (2016)\n4. Man of Steel (2013)\n5. The Avengers (2012)\n6. Iron Man (2008)\n7. Captain Amer...\n\n1. Batman Begins (2005): A young Bruce Wayne becomes Batman to fight crime in Gotham City, facing his fears and training under the guidance of Ra's al Ghul.\n2. The Dark Knight Rises (2012): Batman returns to protect Gotham City from the ruthless terrorist Bane, who plans to destroy the city and its symbol of hope.\n3. Batman v Superman: Dawn of Justice (2016): Batman and Superman clash as their ideologies collide, leading to an epic battle while a new threat emerges that threatens humanity.\n4. Man of Steel (2013): The origin story of Superman, as he embraces his powers and faces General Zod, a fellow Kryptonian seeking to destroy Earth.\n5. The Avengers (2012): Earth's mightiest heroes, including Iron Man, Captain America, Thor, and Hulk, join forces to stop Loki and his alien army from conquering the world.\n6. Iron Man (2008): Billionaire Tony Stark builds a high-tech suit to escape captivity and becomes the superhero Iron Man, using his technology to fight against evil.\n7. Captain America: The Winter Soldier (2014): Captain America teams up with Black Widow and Falcon to uncover a conspiracy within S.H.I.E.L.D. while facing a deadly assassin known as the Winter Soldier.\n8. The Amazing Spider-Man (2012): Peter Parker, a high school student bitten by a radioactive spider, becomes Spider-Man and battles the Lizard, a monstrous villain threatening New York City.\n9. Watchmen (2009): Set in an alternate reality, a group of retired vigilantes investigates the murder of one of their own, uncovering a conspiracy that could have catastrophic consequences.\n10. Sin City (2005): A neo-noir anthology film set in the crime-ridden city of Basin City, following various characters as they navigate through corruption, violence, and redemption.\n11. V for Vendetta (2005): In a dystopian future, a masked vigilante known as V fights against a totalitarian government, inspiring the people to rise up and reclaim their freedom.\n12. Blade Runner 2049 (2017): A young blade runner uncovers a long-buried secret that leads him to seek out former blade runner Rick Deckard, while unraveling the mysteries of a future society.\n13. Inception (2010): A skilled thief enters people's dreams to steal information, but is tasked with planting an idea instead, leading to a mind-bending journey through multiple layers of reality.\n14. The Matrix (1999): A computer hacker discovers the truth about reality, joining a group of rebels fighting against sentient machines that have enslaved humanity in a simulated world.\n15. The Crow (1994): A musician, resurrected by a supernatural crow, seeks vengeance against the gang that murdered him and his fianc\u00e9e, unleashing a dark and atmospheric tale of revenge.</pre> <p>2. Feed streaming output to output parser</p> In\u00a0[\u00a0]: Copied! <pre>p = QueryPipeline(\n    chain=[\n        json_prompt_tmpl,\n        llm.as_query_component(streaming=True),\n        output_parser,\n    ],\n    verbose=True,\n)\noutput = p.run(movie_name=\"Toy Story\")\nprint(output)\n</pre> p = QueryPipeline(     chain=[         json_prompt_tmpl,         llm.as_query_component(streaming=True),         output_parser,     ],     verbose=True, ) output = p.run(movie_name=\"Toy Story\") print(output) <pre>&gt; Running module fe1dbf6a-56e0-44bf-97d7-a2a1fe9d9b8c with input: \nmovie_name: Toy Story\n\n&gt; Running module a8eaaf91-df9d-46c4-bbae-06c15cd15123 with input: \nmessages: Please generate related movies to Toy Story. Output with the following JSON format: \n\n\n\nHere's a JSON schema to follow:\n{\"title\": \"Movies\", \"description\": \"Object representing a list of movies.\", \"typ...\n\n&gt; Running module fcbc0b09-0ef5-43e0-b007-c4508fd6742f with input: \ninput: &lt;generator object llm_chat_callback.&lt;locals&gt;.wrap.&lt;locals&gt;.wrapped_llm_chat.&lt;locals&gt;.wrapped_gen at 0x298d32dc0&gt;\n\nmovies=[Movie(name='Finding Nemo', year=2003), Movie(name='Monsters, Inc.', year=2001), Movie(name='The Incredibles', year=2004), Movie(name='Cars', year=2006), Movie(name='Ratatouille', year=2007)]\n</pre> In\u00a0[\u00a0]: Copied! <pre># !pip install llama-index-postprocessor-cohere-rerank\n</pre> # !pip install llama-index-postprocessor-cohere-rerank In\u00a0[\u00a0]: Copied! <pre>from llama_index.postprocessor.cohere_rerank import CohereRerank\n\n# generate question regarding topic\nprompt_str1 = \"Please generate a concise question about Paul Graham's life regarding the following topic {topic}\"\nprompt_tmpl1 = PromptTemplate(prompt_str1)\n# use HyDE to hallucinate answer.\nprompt_str2 = (\n    \"Please write a passage to answer the question\\n\"\n    \"Try to include as many key details as possible.\\n\"\n    \"\\n\"\n    \"\\n\"\n    \"{query_str}\\n\"\n    \"\\n\"\n    \"\\n\"\n    'Passage:\"\"\"\\n'\n)\nprompt_tmpl2 = PromptTemplate(prompt_str2)\n\nllm = OpenAI(model=\"gpt-3.5-turbo\")\nretriever = index.as_retriever(similarity_top_k=5)\np = QueryPipeline(\n    chain=[prompt_tmpl1, llm, prompt_tmpl2, llm, retriever], verbose=True\n)\n</pre> from llama_index.postprocessor.cohere_rerank import CohereRerank  # generate question regarding topic prompt_str1 = \"Please generate a concise question about Paul Graham's life regarding the following topic {topic}\" prompt_tmpl1 = PromptTemplate(prompt_str1) # use HyDE to hallucinate answer. prompt_str2 = (     \"Please write a passage to answer the question\\n\"     \"Try to include as many key details as possible.\\n\"     \"\\n\"     \"\\n\"     \"{query_str}\\n\"     \"\\n\"     \"\\n\"     'Passage:\"\"\"\\n' ) prompt_tmpl2 = PromptTemplate(prompt_str2)  llm = OpenAI(model=\"gpt-3.5-turbo\") retriever = index.as_retriever(similarity_top_k=5) p = QueryPipeline(     chain=[prompt_tmpl1, llm, prompt_tmpl2, llm, retriever], verbose=True ) In\u00a0[\u00a0]: Copied! <pre>nodes = p.run(topic=\"college\")\nlen(nodes)\n</pre> nodes = p.run(topic=\"college\") len(nodes) <pre>&gt; Running module f5435516-61b6-49e9-9926-220cfb6443bd with input: \ntopic: college\n\n&gt; Running module 1dcaa097-cedc-4466-81bb-f8fd8768762b with input: \nmessages: Please generate a concise question about Paul Graham's life regarding the following topic college\n\n&gt; Running module 891afa10-5fe0-47ed-bdee-42a59d0e916d with input: \nquery_str: assistant: How did Paul Graham's college experience shape his career and entrepreneurial mindset?\n\n&gt; Running module 5bcd9964-b972-41a9-960d-96894c57a372 with input: \nmessages: Please write a passage to answer the question\nTry to include as many key details as possible.\n\n\nHow did Paul Graham's college experience shape his career and entrepreneurial mindset?\n\n\nPassage:\"\"\"\n\n\n&gt; Running module 0b81a91a-2c90-4700-8ba1-25ffad5311fd with input: \ninput: assistant: Paul Graham's college experience played a pivotal role in shaping his career and entrepreneurial mindset. As a student at Cornell University, Graham immersed himself in the world of compute...\n\n</pre> Out[\u00a0]: <pre>5</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.postprocessor.cohere_rerank import CohereRerank\nfrom llama_index.core.response_synthesizers import TreeSummarize\n\n\n# define modules\nprompt_str = \"Please generate a question about Paul Graham's life regarding the following topic {topic}\"\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\nretriever = index.as_retriever(similarity_top_k=3)\nreranker = CohereRerank()\nsummarizer = TreeSummarize(llm=llm)\n</pre> from llama_index.postprocessor.cohere_rerank import CohereRerank from llama_index.core.response_synthesizers import TreeSummarize   # define modules prompt_str = \"Please generate a question about Paul Graham's life regarding the following topic {topic}\" prompt_tmpl = PromptTemplate(prompt_str) llm = OpenAI(model=\"gpt-3.5-turbo\") retriever = index.as_retriever(similarity_top_k=3) reranker = CohereRerank() summarizer = TreeSummarize(llm=llm) In\u00a0[\u00a0]: Copied! <pre># define query pipeline\np = QueryPipeline(verbose=True)\np.add_modules(\n    {\n        \"llm\": llm,\n        \"prompt_tmpl\": prompt_tmpl,\n        \"retriever\": retriever,\n        \"summarizer\": summarizer,\n        \"reranker\": reranker,\n    }\n)\n</pre> # define query pipeline p = QueryPipeline(verbose=True) p.add_modules(     {         \"llm\": llm,         \"prompt_tmpl\": prompt_tmpl,         \"retriever\": retriever,         \"summarizer\": summarizer,         \"reranker\": reranker,     } ) <p>Next we draw links between modules with <code>add_link</code>. <code>add_link</code> takes in the source/destination module ids, and optionally the <code>source_key</code> and <code>dest_key</code>. Specify the <code>source_key</code> or <code>dest_key</code> if there are multiple outputs/inputs respectively.</p> <p>You can view the set of input/output keys for each module through <code>module.as_query_component().input_keys</code> and <code>module.as_query_component().output_keys</code>.</p> <p>Here we explicitly specify <code>dest_key</code> for the <code>reranker</code> and <code>summarizer</code> modules because they take in two inputs (query_str and nodes).</p> In\u00a0[\u00a0]: Copied! <pre>p.add_link(\"prompt_tmpl\", \"llm\")\np.add_link(\"llm\", \"retriever\")\np.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\")\np.add_link(\"llm\", \"reranker\", dest_key=\"query_str\")\np.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\np.add_link(\"llm\", \"summarizer\", dest_key=\"query_str\")\n\n# look at summarizer input keys\nprint(summarizer.as_query_component().input_keys)\n</pre> p.add_link(\"prompt_tmpl\", \"llm\") p.add_link(\"llm\", \"retriever\") p.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\") p.add_link(\"llm\", \"reranker\", dest_key=\"query_str\") p.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\") p.add_link(\"llm\", \"summarizer\", dest_key=\"query_str\")  # look at summarizer input keys print(summarizer.as_query_component().input_keys) <pre>required_keys={'query_str', 'nodes'} optional_keys=set()\n</pre> <p>We use <code>networkx</code> to store the graph representation. This gives us an easy way to view the DAG!</p> In\u00a0[\u00a0]: Copied! <pre>## create graph\nfrom pyvis.network import Network\n\nnet = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\nnet.from_nx(p.dag)\nnet.show(\"rag_dag.html\")\n\n## another option using `pygraphviz`\n# from networkx.drawing.nx_agraph import to_agraph\n# from IPython.display import Image\n# agraph = to_agraph(p.dag)\n# agraph.layout(prog=\"dot\")\n# agraph.draw('rag_dag.png')\n# display(Image('rag_dag.png'))\n</pre> ## create graph from pyvis.network import Network  net = Network(notebook=True, cdn_resources=\"in_line\", directed=True) net.from_nx(p.dag) net.show(\"rag_dag.html\")  ## another option using `pygraphviz` # from networkx.drawing.nx_agraph import to_agraph # from IPython.display import Image # agraph = to_agraph(p.dag) # agraph.layout(prog=\"dot\") # agraph.draw('rag_dag.png') # display(Image('rag_dag.png')) <pre>rag_dag.html\n</pre> Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>response = p.run(topic=\"YC\")\n</pre> response = p.run(topic=\"YC\") <pre>&gt; Running module prompt_tmpl with input: \ntopic: YC\n\n&gt; Running module llm with input: \nmessages: Please generate a question about Paul Graham's life regarding the following topic YC\n\n&gt; Running module retriever with input: \ninput: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n\n&gt; Running module reranker with input: \nquery_str: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\nnodes: [NodeWithScore(node=TextNode(id_='ccd39041-5a64-4bd3-aca7-48f804b5a23f', embedding=None, metadata={'file_path': '../data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file...\n\n&gt; Running module summarizer with input: \nquery_str: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\nnodes: [NodeWithScore(node=TextNode(id_='120574dd-a5c9-4985-ab3e-37b1070b500a', embedding=None, metadata={'file_path': '../data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file...\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(str(response))\n</pre> print(str(response)) <pre>Paul Graham played a significant role in the founding and development of Y Combinator (YC). He was one of the co-founders of YC and provided the initial funding for the investment firm. Along with his partners, he implemented the ideas they had been discussing and started their own investment firm. Paul Graham also played a key role in shaping the unique batch model of YC, where a group of startups is funded and provided intensive support for a period of three months. He was actively involved in selecting and helping the founders, and he also wrote essays and worked on YC's internal software.\n</pre> In\u00a0[\u00a0]: Copied! <pre># you can do async too\nresponse = await p.arun(topic=\"YC\")\nprint(str(response))\n</pre> # you can do async too response = await p.arun(topic=\"YC\") print(str(response)) <pre>&gt; Running modules and inputs in parallel: \nModule key: prompt_tmpl. Input: \ntopic: YC\n\n\n&gt; Running modules and inputs in parallel: \nModule key: llm. Input: \nmessages: Please generate a question about Paul Graham's life regarding the following topic YC\n\n\n&gt; Running modules and inputs in parallel: \nModule key: retriever. Input: \ninput: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n\n\n&gt; Running modules and inputs in parallel: \nModule key: reranker. Input: \nquery_str: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\nnodes: [NodeWithScore(node=TextNode(id_='ccd39041-5a64-4bd3-aca7-48f804b5a23f', embedding=None, metadata={'file_path': '../data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file...\n\n\n&gt; Running modules and inputs in parallel: \nModule key: summarizer. Input: \nquery_str: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\nnodes: [NodeWithScore(node=TextNode(id_='120574dd-a5c9-4985-ab3e-37b1070b500a', embedding=None, metadata={'file_path': '../data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file...\n\n\nPaul Graham played a significant role in the founding and development of Y Combinator (YC). He was one of the co-founders of YC and provided the initial funding for the investment firm. Along with his partners, he implemented the ideas they had been discussing and decided to start their own investment firm. Paul Graham also played a key role in shaping the unique batch model of YC, where a group of startups is funded and provided intensive support for a period of three months. He was actively involved in selecting and helping the founders and worked on various projects related to YC, including writing essays and developing internal software.\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.postprocessor.cohere_rerank import CohereRerank\nfrom llama_index.core.response_synthesizers import TreeSummarize\nfrom llama_index.core.query_pipeline import InputComponent\n\nretriever = index.as_retriever(similarity_top_k=5)\nsummarizer = TreeSummarize(llm=OpenAI(model=\"gpt-3.5-turbo\"))\nreranker = CohereRerank()\n</pre> from llama_index.postprocessor.cohere_rerank import CohereRerank from llama_index.core.response_synthesizers import TreeSummarize from llama_index.core.query_pipeline import InputComponent  retriever = index.as_retriever(similarity_top_k=5) summarizer = TreeSummarize(llm=OpenAI(model=\"gpt-3.5-turbo\")) reranker = CohereRerank() In\u00a0[\u00a0]: Copied! <pre>p = QueryPipeline(verbose=True)\np.add_modules(\n    {\n        \"input\": InputComponent(),\n        \"retriever\": retriever,\n        \"summarizer\": summarizer,\n    }\n)\np.add_link(\"input\", \"retriever\")\np.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")\np.add_link(\"retriever\", \"summarizer\", dest_key=\"nodes\")\n</pre> p = QueryPipeline(verbose=True) p.add_modules(     {         \"input\": InputComponent(),         \"retriever\": retriever,         \"summarizer\": summarizer,     } ) p.add_link(\"input\", \"retriever\") p.add_link(\"input\", \"summarizer\", dest_key=\"query_str\") p.add_link(\"retriever\", \"summarizer\", dest_key=\"nodes\") In\u00a0[\u00a0]: Copied! <pre>output = p.run(input=\"what did the author do in YC\")\n</pre> output = p.run(input=\"what did the author do in YC\") <pre>&gt; Running module input with input: \ninput: what did the author do in YC\n\n&gt; Running module retriever with input: \ninput: what did the author do in YC\n\n&gt; Running module summarizer with input: \nquery_str: what did the author do in YC\nnodes: [NodeWithScore(node=TextNode(id_='86dea730-ca35-4bcb-9f9b-4c99e8eadd08', embedding=None, metadata={'file_path': '../data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file...\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(str(output))\n</pre> print(str(output)) <pre>The author worked on various projects at YC, including writing essays and working on YC's internal software. They also played a key role in the creation and operation of YC by funding the program with their own money and organizing a batch model where they would fund a group of startups twice a year. They provided support and guidance to the startups during a three-month intensive program and used their building in Cambridge as the headquarters for YC. Additionally, they hosted weekly dinners where experts on startups would give talks.\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.query_pipeline import (\n    CustomQueryComponent,\n    InputKeys,\n    OutputKeys,\n)\nfrom typing import Dict, Any\nfrom llama_index.core.llms.llm import LLM\nfrom pydantic import Field\n\n\nclass RelatedMovieComponent(CustomQueryComponent):\n    \"\"\"Related movie component.\"\"\"\n\n    llm: LLM = Field(..., description=\"OpenAI LLM\")\n\n    def _validate_component_inputs(\n        self, input: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        # NOTE: this is OPTIONAL but we show you here how to do validation as an example\n        return input\n\n    @property\n    def _input_keys(self) -&gt; set:\n        \"\"\"Input keys dict.\"\"\"\n        # NOTE: These are required inputs. If you have optional inputs please override\n        # `optional_input_keys_dict`\n        return {\"movie\"}\n\n    @property\n    def _output_keys(self) -&gt; set:\n        return {\"output\"}\n\n    def _run_component(self, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"Run the component.\"\"\"\n        # use QueryPipeline itself here for convenience\n        prompt_str = \"Please generate related movies to {movie_name}\"\n        prompt_tmpl = PromptTemplate(prompt_str)\n        p = QueryPipeline(chain=[prompt_tmpl, llm])\n        return {\"output\": p.run(movie_name=kwargs[\"movie\"])}\n</pre> from llama_index.core.query_pipeline import (     CustomQueryComponent,     InputKeys,     OutputKeys, ) from typing import Dict, Any from llama_index.core.llms.llm import LLM from pydantic import Field   class RelatedMovieComponent(CustomQueryComponent):     \"\"\"Related movie component.\"\"\"      llm: LLM = Field(..., description=\"OpenAI LLM\")      def _validate_component_inputs(         self, input: Dict[str, Any]     ) -&gt; Dict[str, Any]:         \"\"\"Validate component inputs during run_component.\"\"\"         # NOTE: this is OPTIONAL but we show you here how to do validation as an example         return input      @property     def _input_keys(self) -&gt; set:         \"\"\"Input keys dict.\"\"\"         # NOTE: These are required inputs. If you have optional inputs please override         # `optional_input_keys_dict`         return {\"movie\"}      @property     def _output_keys(self) -&gt; set:         return {\"output\"}      def _run_component(self, **kwargs) -&gt; Dict[str, Any]:         \"\"\"Run the component.\"\"\"         # use QueryPipeline itself here for convenience         prompt_str = \"Please generate related movies to {movie_name}\"         prompt_tmpl = PromptTemplate(prompt_str)         p = QueryPipeline(chain=[prompt_tmpl, llm])         return {\"output\": p.run(movie_name=kwargs[\"movie\"])} <p>Let's try the custom component out! We'll also add a step to convert the output to Shakespeare.</p> In\u00a0[\u00a0]: Copied! <pre>llm = OpenAI(model=\"gpt-3.5-turbo\")\ncomponent = RelatedMovieComponent(llm=llm)\n\n# let's add some subsequent prompts for fun\nprompt_str = \"\"\"\\\nHere's some text:\n\n{text}\n\nCan you rewrite this in the voice of Shakespeare?\n\"\"\"\nprompt_tmpl = PromptTemplate(prompt_str)\n\np = QueryPipeline(chain=[component, prompt_tmpl, llm], verbose=True)\n</pre> llm = OpenAI(model=\"gpt-3.5-turbo\") component = RelatedMovieComponent(llm=llm)  # let's add some subsequent prompts for fun prompt_str = \"\"\"\\ Here's some text:  {text}  Can you rewrite this in the voice of Shakespeare? \"\"\" prompt_tmpl = PromptTemplate(prompt_str)  p = QueryPipeline(chain=[component, prompt_tmpl, llm], verbose=True) In\u00a0[\u00a0]: Copied! <pre>output = p.run(movie=\"Love Actually\")\n</pre> output = p.run(movie=\"Love Actually\") <pre>&gt; Running module 31ca224a-f226-4956-882b-73878843d869 with input: \nmovie: Love Actually\n\n&gt; Running module febb41b5-2528-416a-bde7-6accdb0f9c51 with input: \ntext: assistant: 1. \"Valentine's Day\" (2010)\n2. \"New Year's Eve\" (2011)\n3. \"The Holiday\" (2006)\n4. \"Crazy, Stupid, Love\" (2011)\n5. \"Notting Hill\" (1999)\n6. \"Four Weddings and a Funeral\" (1994)\n7. \"Bridget J...\n\n&gt; Running module e834ffbe-e97c-4ab0-9726-24f1534745b2 with input: \nmessages: Here's some text:\n\n1. \"Valentine's Day\" (2010)\n2. \"New Year's Eve\" (2011)\n3. \"The Holiday\" (2006)\n4. \"Crazy, Stupid, Love\" (2011)\n5. \"Notting Hill\" (1999)\n6. \"Four Weddings and a Funeral\" (1994)\n7. \"B...\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(str(output))\n</pre> print(str(output)) <pre>assistant: 1. \"Valentine's Day\" (2010) - \"A day of love, where hearts entwine, \n   And Cupid's arrow finds its mark divine.\"\n\n2. \"New Year's Eve\" (2011) - \"When old year fades, and new year dawns,\n   We gather 'round, to celebrate the morns.\"\n\n3. \"The Holiday\" (2006) - \"Two souls, adrift in search of cheer,\n   Find solace in a holiday so dear.\"\n\n4. \"Crazy, Stupid, Love\" (2011) - \"A tale of love, both wild and mad,\n   Where hearts are lost, then found, and glad.\"\n\n5. \"Notting Hill\" (1999) - \"In London town, where love may bloom,\n   A humble man finds love, and breaks the gloom.\"\n\n6. \"Four Weddings and a Funeral\" (1994) - \"Four times the vows, and one time mourn,\n   Love's journey, with laughter and tears adorned.\"\n\n7. \"Bridget Jones's Diary\" (2001) - \"A maiden fair, with wit and charm,\n   Records her life, and love's alarm.\"\n\n8. \"About Time\" (2013) - \"A tale of time, where love transcends,\n   And moments cherished, never truly ends.\"\n\n9. \"The Best Exotic Marigold Hotel\" (2011) - \"In India's land, where dreams unfold,\n   A hotel blooms, where hearts find gold.\"\n\n10. \"The Notebook\" (2004) - \"A love that spans both time and space,\n    Where words and memories find their place.\"\n\n11. \"Serendipity\" (2001) - \"By chance or fate, two souls collide,\n    In search of love, they cannot hide.\"\n\n12. \"P.S. I Love You\" (2007) - \"In letters penned, from love's embrace,\n    A departed soul, still finds its trace.\"\n\n13. \"500 Days of Summer\" (2009) - \"A tale of love, both sweet and sour,\n    Where seasons change, and hearts devour.\"\n\n14. \"The Fault in Our Stars\" (2014) - \"Two hearts, aflame, in starlit skies,\n    Love's tragedy, where hope never dies.\"\n\n15. \"La La Land\" (2016) - \"In dreams and songs, two hearts entwine,\n    A city's magic, where love's stars align.\"\n</pre>"},{"location":"examples/pipeline/query_pipeline/#an-introduction-to-llamaindex-query-pipelines","title":"An Introduction to LlamaIndex Query Pipelines\u00b6","text":""},{"location":"examples/pipeline/query_pipeline/#overview","title":"Overview\u00b6","text":"<p>LlamaIndex provides a declarative query API that allows you to chain together different modules in order to orchestrate simple-to-advanced workflows over your data.</p> <p>This is centered around our <code>QueryPipeline</code> abstraction. Load in a variety of modules (from LLMs to prompts to retrievers to other pipelines), connect them all together into a sequential chain or DAG, and run it end2end.</p> <p>NOTE: You can orchestrate all these workflows without the declarative pipeline abstraction (by using the modules imperatively and writing your own functions). So what are the advantages of <code>QueryPipeline</code>?</p> <ul> <li>Express common workflows with fewer lines of code/boilerplate</li> <li>Greater readability</li> <li>Greater parity / better integration points with common low-code / no-code solutions (e.g. LangFlow)</li> <li>[In the future] A declarative interface allows easy serializability of pipeline components, providing portability of pipelines/easier deployment to different systems.</li> </ul>"},{"location":"examples/pipeline/query_pipeline/#cookbook","title":"Cookbook\u00b6","text":"<p>In this cookbook we give you an introduction to our <code>QueryPipeline</code> interface and show you some basic workflows you can tackle.</p> <ul> <li>Chain together prompt and LLM</li> <li>Chain together query rewriting (prompt + LLM) with retrieval</li> <li>Chain together a full RAG query pipeline (query rewriting, retrieval, reranking, response synthesis)</li> <li>Setting up a custom query component</li> </ul>"},{"location":"examples/pipeline/query_pipeline/#setup","title":"Setup\u00b6","text":"<p>Here we setup some data + indexes (from PG's essay) that we'll be using in the rest of the cookbook.</p>"},{"location":"examples/pipeline/query_pipeline/#1-chain-together-prompt-and-llm","title":"1. Chain Together Prompt and LLM\u00b6","text":"<p>In this section we show a super simple workflow of chaining together a prompt with LLM.</p> <p>We simply define <code>chain</code> on initialization. This is a special case of a query pipeline where the components are purely sequential, and we automatically convert outputs into the right format for the next inputs.</p>"},{"location":"examples/pipeline/query_pipeline/#try-output-parsing","title":"Try Output Parsing\u00b6","text":"<p>Let's parse the outputs into a structured Pydantic object.</p>"},{"location":"examples/pipeline/query_pipeline/#streaming-support","title":"Streaming Support\u00b6","text":"<p>The query pipelines have LLM streaming support (simply do <code>as_query_component(streaming=True)</code>). Intermediate outputs will get autoconverted, and the final output can be a streaming output. Here's some examples.</p>"},{"location":"examples/pipeline/query_pipeline/#chain-together-query-rewriting-workflow-prompts-llm-with-retrieval","title":"Chain Together Query Rewriting Workflow (prompts + LLM) with Retrieval\u00b6","text":"<p>Here we try a slightly more complex workflow where we send the input through two prompts before initiating retrieval.</p> <ol> <li>Generate question about given topic.</li> <li>Hallucinate answer given question, for better retrieval.</li> </ol> <p>Since each prompt only takes in one input, note that the <code>QueryPipeline</code> will automatically chain LLM outputs into the prompt and then into the LLM.</p> <p>You'll see how to define links more explicitly in the next section.</p>"},{"location":"examples/pipeline/query_pipeline/#create-a-full-rag-pipeline-as-a-dag","title":"Create a Full RAG Pipeline as a DAG\u00b6","text":"<p>Here we chain together a full RAG pipeline consisting of query rewriting, retrieval, reranking, and response synthesis.</p> <p>Here we can't use <code>chain</code> syntax because certain modules depend on multiple inputs (for instance, response synthesis expects both the retrieved nodes and the original question). Instead we'll construct a DAG explicitly, through <code>add_modules</code> and then <code>add_link</code>.</p>"},{"location":"examples/pipeline/query_pipeline/#1-rag-pipeline-with-query-rewriting","title":"1. RAG Pipeline with Query Rewriting\u00b6","text":"<p>We use an LLM to rewrite the query first before passing it to our downstream modules - retrieval/reranking/synthesis.</p>"},{"location":"examples/pipeline/query_pipeline/#2-rag-pipeline-without-query-rewriting","title":"2. RAG Pipeline without Query Rewriting\u00b6","text":"<p>Here we setup a RAG pipeline without the query rewriting step.</p> <p>Here we need a way to link the input query to both the retriever, reranker, and summarizer. We can do this by defining a special <code>InputComponent</code>, allowing us to link the inputs to multiple downstream modules.</p>"},{"location":"examples/pipeline/query_pipeline/#defining-a-custom-component-in-a-query-pipeline","title":"Defining a Custom Component in a Query Pipeline\u00b6","text":"<p>You can easily define a custom component. Simply subclass a <code>QueryComponent</code>, implement validation/run functions + some helpers, and plug it in.</p> <p>Let's wrap the related movie generation prompt+LLM chain from the first example into a custom component.</p>"},{"location":"examples/pipeline/query_pipeline_pandas/","title":"Query Pipeline over Pandas DataFrames","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-llms-openai llama-index-experimental\n</pre> %pip install llama-index-llms-openai llama-index-experimental In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.query_pipeline import (\n    QueryPipeline as QP,\n    Link,\n    InputComponent,\n)\nfrom llama_index.experimental.query_engine.pandas import (\n    PandasInstructionParser,\n)\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import PromptTemplate\n</pre> from llama_index.core.query_pipeline import (     QueryPipeline as QP,     Link,     InputComponent, ) from llama_index.experimental.query_engine.pandas import (     PandasInstructionParser, ) from llama_index.llms.openai import OpenAI from llama_index.core import PromptTemplate In\u00a0[\u00a0]: Copied! <pre>!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/docs/examples/data/csv/titanic_train.csv' -O 'titanic_train.csv'\n</pre> !wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/docs/examples/data/csv/titanic_train.csv' -O 'titanic_train.csv' <pre>--2024-01-13 18:39:07--  https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/docs/examples/data/csv/titanic_train.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 57726 (56K) [text/plain]\nSaving to: \u2018titanic_train.csv\u2019\n\ntitanic_train.csv   100%[===================&gt;]  56.37K  --.-KB/s    in 0.007s  \n\n2024-01-13 18:39:07 (7.93 MB/s) - \u2018titanic_train.csv\u2019 saved [57726/57726]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"./titanic_train.csv\")\n</pre> import pandas as pd  df = pd.read_csv(\"./titanic_train.csv\") In\u00a0[\u00a0]: Copied! <pre>instruction_str = (\n    \"1. Convert the query to executable Python code using Pandas.\\n\"\n    \"2. The final line of code should be a Python expression that can be called with the `eval()` function.\\n\"\n    \"3. The code should represent a solution to the query.\\n\"\n    \"4. PRINT ONLY THE EXPRESSION.\\n\"\n    \"5. Do not quote the expression.\\n\"\n)\n\npandas_prompt_str = (\n    \"You are working with a pandas dataframe in Python.\\n\"\n    \"The name of the dataframe is `df`.\\n\"\n    \"This is the result of `print(df.head())`:\\n\"\n    \"{df_str}\\n\\n\"\n    \"Follow these instructions:\\n\"\n    \"{instruction_str}\\n\"\n    \"Query: {query_str}\\n\\n\"\n    \"Expression:\"\n)\nresponse_synthesis_prompt_str = (\n    \"Given an input question, synthesize a response from the query results.\\n\"\n    \"Query: {query_str}\\n\\n\"\n    \"Pandas Instructions (optional):\\n{pandas_instructions}\\n\\n\"\n    \"Pandas Output: {pandas_output}\\n\\n\"\n    \"Response: \"\n)\n\npandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n    instruction_str=instruction_str, df_str=df.head(5)\n)\npandas_output_parser = PandasInstructionParser(df)\nresponse_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n</pre> instruction_str = (     \"1. Convert the query to executable Python code using Pandas.\\n\"     \"2. The final line of code should be a Python expression that can be called with the `eval()` function.\\n\"     \"3. The code should represent a solution to the query.\\n\"     \"4. PRINT ONLY THE EXPRESSION.\\n\"     \"5. Do not quote the expression.\\n\" )  pandas_prompt_str = (     \"You are working with a pandas dataframe in Python.\\n\"     \"The name of the dataframe is `df`.\\n\"     \"This is the result of `print(df.head())`:\\n\"     \"{df_str}\\n\\n\"     \"Follow these instructions:\\n\"     \"{instruction_str}\\n\"     \"Query: {query_str}\\n\\n\"     \"Expression:\" ) response_synthesis_prompt_str = (     \"Given an input question, synthesize a response from the query results.\\n\"     \"Query: {query_str}\\n\\n\"     \"Pandas Instructions (optional):\\n{pandas_instructions}\\n\\n\"     \"Pandas Output: {pandas_output}\\n\\n\"     \"Response: \" )  pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(     instruction_str=instruction_str, df_str=df.head(5) ) pandas_output_parser = PandasInstructionParser(df) response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str) llm = OpenAI(model=\"gpt-3.5-turbo\") In\u00a0[\u00a0]: Copied! <pre>qp = QP(\n    modules={\n        \"input\": InputComponent(),\n        \"pandas_prompt\": pandas_prompt,\n        \"llm1\": llm,\n        \"pandas_output_parser\": pandas_output_parser,\n        \"response_synthesis_prompt\": response_synthesis_prompt,\n        \"llm2\": llm,\n    },\n    verbose=True,\n)\nqp.add_chain([\"input\", \"pandas_prompt\", \"llm1\", \"pandas_output_parser\"])\nqp.add_links(\n    [\n        Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n        Link(\n            \"llm1\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"\n        ),\n        Link(\n            \"pandas_output_parser\",\n            \"response_synthesis_prompt\",\n            dest_key=\"pandas_output\",\n        ),\n    ]\n)\n# add link from response synthesis prompt to llm2\nqp.add_link(\"response_synthesis_prompt\", \"llm2\")\n</pre> qp = QP(     modules={         \"input\": InputComponent(),         \"pandas_prompt\": pandas_prompt,         \"llm1\": llm,         \"pandas_output_parser\": pandas_output_parser,         \"response_synthesis_prompt\": response_synthesis_prompt,         \"llm2\": llm,     },     verbose=True, ) qp.add_chain([\"input\", \"pandas_prompt\", \"llm1\", \"pandas_output_parser\"]) qp.add_links(     [         Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),         Link(             \"llm1\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"         ),         Link(             \"pandas_output_parser\",             \"response_synthesis_prompt\",             dest_key=\"pandas_output\",         ),     ] ) # add link from response synthesis prompt to llm2 qp.add_link(\"response_synthesis_prompt\", \"llm2\") In\u00a0[\u00a0]: Copied! <pre>response = qp.run(\n    query_str=\"What is the correlation between survival and age?\",\n)\n</pre> response = qp.run(     query_str=\"What is the correlation between survival and age?\", ) <pre>&gt; Running module input with input: \nquery_str: What is the correlation between survival and age?\n\n&gt; Running module pandas_prompt with input: \nquery_str: What is the correlation between survival and age?\n\n&gt; Running module llm1 with input: \nmessages: You are working with a pandas dataframe in Python.\nThe name of the dataframe is `df`.\nThis is the result of `print(df.head())`:\n   survived  pclass                                               name  ...\n\n&gt; Running module pandas_output_parser with input: \ninput: assistant: df['survived'].corr(df['age'])\n\n&gt; Running module response_synthesis_prompt with input: \nquery_str: What is the correlation between survival and age?\npandas_instructions: assistant: df['survived'].corr(df['age'])\npandas_output: -0.07722109457217755\n\n&gt; Running module llm2 with input: \nmessages: Given an input question, synthesize a response from the query results.\nQuery: What is the correlation between survival and age?\n\nPandas Instructions (optional):\ndf['survived'].corr(df['age'])\n\nPandas ...\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(response.message.content)\n</pre> print(response.message.content) <pre>The correlation between survival and age is -0.0772. This indicates a weak negative correlation, suggesting that as age increases, the likelihood of survival slightly decreases.\n</pre>"},{"location":"examples/pipeline/query_pipeline_pandas/#query-pipeline-over-pandas-dataframes","title":"Query Pipeline over Pandas DataFrames\u00b6","text":"<p>This is a simple example that builds a query pipeline that can perform structured operations over a Pandas DataFrame to satisfy a user query, using LLMs to infer the set of operations.</p> <p>This can be treated as the \"from-scratch\" version of our <code>PandasQueryEngine</code>.</p> <p>WARNING: This tool provides the LLM access to the <code>eval</code> function. Arbitrary code execution is possible on the machine running this tool. This tool is not recommended to be used in a production setting, and would require heavy sandboxing or virtual machines.</p>"},{"location":"examples/pipeline/query_pipeline_pandas/#download-data","title":"Download Data\u00b6","text":"<p>Here we load the Titanic CSV dataset.</p>"},{"location":"examples/pipeline/query_pipeline_pandas/#define-modules","title":"Define Modules\u00b6","text":"<p>Here we define the set of modules:</p> <ol> <li>Pandas prompt to infer pandas instructions from user query</li> <li>Pandas output parser to execute pandas instructions on dataframe, get back dataframe</li> <li>Response synthesis prompt to synthesize a final response given the dataframe</li> <li>LLM</li> </ol> <p>The pandas output parser specifically is designed to safely execute Python code. It includes a lot of safety checks that may be annoying to write from scratch. This includes only importing from a set of approved modules (e.g. no modules that would alter the file system like <code>os</code>), and also making sure that no private/dunder methods are being called.</p>"},{"location":"examples/pipeline/query_pipeline_pandas/#build-query-pipeline","title":"Build Query Pipeline\u00b6","text":"<p>Looks like this: input query_str -&gt; pandas_prompt -&gt; llm1 -&gt; pandas_output_parser -&gt; response_synthesis_prompt -&gt; llm2</p> <p>Additional connections to response_synthesis_prompt: llm1 -&gt; pandas_instructions, and pandas_output_parser -&gt; pandas_output.</p>"},{"location":"examples/pipeline/query_pipeline_pandas/#run-query","title":"Run Query\u00b6","text":""},{"location":"examples/pipeline/query_pipeline_routing/","title":"Query Pipeline with Routing","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-llms-openai\n</pre> %pip install llama-index-llms-openai In\u00a0[\u00a0]: Copied! <pre>!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt\n</pre> !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt <pre>--2024-01-10 12:31:00--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75042 (73K) [text/plain]\nSaving to: \u2018pg_essay.txt\u2019\n\npg_essay.txt        100%[===================&gt;]  73.28K  --.-KB/s    in 0.01s   \n\n2024-01-10 12:31:00 (6.32 MB/s) - \u2018pg_essay.txt\u2019 saved [75042/75042]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import SimpleDirectoryReader\n\nreader = SimpleDirectoryReader(input_files=[\"pg_essay.txt\"])\ndocuments = reader.load_data()\n</pre> from llama_index.core import SimpleDirectoryReader  reader = SimpleDirectoryReader(input_files=[\"pg_essay.txt\"]) documents = reader.load_data() In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.query_pipeline import QueryPipeline, InputComponent\nfrom typing import Dict, Any, List, Optional\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Document, VectorStoreIndex\nfrom llama_index.core import SummaryIndex\nfrom llama_index.core.response_synthesizers import TreeSummarize\nfrom llama_index.core.schema import NodeWithScore, TextNode\nfrom llama_index.core import PromptTemplate\nfrom llama_index.core.selectors import LLMSingleSelector\n\n# define HyDE template\nhyde_str = \"\"\"\\\nPlease write a passage to answer the question: {query_str}\n\nTry to include as many key details as possible.\n\nPassage: \"\"\"\nhyde_prompt = PromptTemplate(hyde_str)\n\n# define llm\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\n\n# define synthesizer\nsummarizer = TreeSummarize(llm=llm)\n\n# define vector retriever\nvector_index = VectorStoreIndex.from_documents(documents)\nvector_query_engine = vector_index.as_query_engine(similarity_top_k=2)\n\n# define summary query prompts + retrievers\nsummary_index = SummaryIndex.from_documents(documents)\nsummary_qrewrite_str = \"\"\"\\\nHere's a question:\n{query_str}\n\nYou are responsible for feeding the question to an agent that given context will try to answer the question.\nThe context may or may not be relevant. Rewrite the question to highlight the fact that\nonly some pieces of context (or none) maybe be relevant.\n\"\"\"\nsummary_qrewrite_prompt = PromptTemplate(summary_qrewrite_str)\nsummary_query_engine = summary_index.as_query_engine()\n\n# define selector\nselector = LLMSingleSelector.from_defaults()\n</pre> from llama_index.core.query_pipeline import QueryPipeline, InputComponent from typing import Dict, Any, List, Optional from llama_index.llms.openai import OpenAI from llama_index.core import Document, VectorStoreIndex from llama_index.core import SummaryIndex from llama_index.core.response_synthesizers import TreeSummarize from llama_index.core.schema import NodeWithScore, TextNode from llama_index.core import PromptTemplate from llama_index.core.selectors import LLMSingleSelector  # define HyDE template hyde_str = \"\"\"\\ Please write a passage to answer the question: {query_str}  Try to include as many key details as possible.  Passage: \"\"\" hyde_prompt = PromptTemplate(hyde_str)  # define llm llm = OpenAI(model=\"gpt-3.5-turbo\")   # define synthesizer summarizer = TreeSummarize(llm=llm)  # define vector retriever vector_index = VectorStoreIndex.from_documents(documents) vector_query_engine = vector_index.as_query_engine(similarity_top_k=2)  # define summary query prompts + retrievers summary_index = SummaryIndex.from_documents(documents) summary_qrewrite_str = \"\"\"\\ Here's a question: {query_str}  You are responsible for feeding the question to an agent that given context will try to answer the question. The context may or may not be relevant. Rewrite the question to highlight the fact that only some pieces of context (or none) maybe be relevant. \"\"\" summary_qrewrite_prompt = PromptTemplate(summary_qrewrite_str) summary_query_engine = summary_index.as_query_engine()  # define selector selector = LLMSingleSelector.from_defaults() In\u00a0[\u00a0]: Copied! <pre># define summary query pipeline\nfrom llama_index.core.query_pipeline import RouterComponent\n\nvector_chain = QueryPipeline(chain=[vector_query_engine])\nsummary_chain = QueryPipeline(\n    chain=[summary_qrewrite_prompt, llm, summary_query_engine], verbose=True\n)\n\nchoices = [\n    \"This tool answers specific questions about the document (not summary questions across the document)\",\n    \"This tool answers summary questions about the document (not specific questions)\",\n]\n\nrouter_c = RouterComponent(\n    selector=selector,\n    choices=choices,\n    components=[vector_chain, summary_chain],\n    verbose=True,\n)\n# top-level pipeline\nqp = QueryPipeline(chain=[router_c], verbose=True)\n</pre> # define summary query pipeline from llama_index.core.query_pipeline import RouterComponent  vector_chain = QueryPipeline(chain=[vector_query_engine]) summary_chain = QueryPipeline(     chain=[summary_qrewrite_prompt, llm, summary_query_engine], verbose=True )  choices = [     \"This tool answers specific questions about the document (not summary questions across the document)\",     \"This tool answers summary questions about the document (not specific questions)\", ]  router_c = RouterComponent(     selector=selector,     choices=choices,     components=[vector_chain, summary_chain],     verbose=True, ) # top-level pipeline qp = QueryPipeline(chain=[router_c], verbose=True) In\u00a0[\u00a0]: Copied! <pre># compare with sync method\nresponse = qp.run(\"What did the author do during his time in YC?\")\nprint(str(response))\n</pre> # compare with sync method response = qp.run(\"What did the author do during his time in YC?\") print(str(response)) <pre>&gt; Running module c0a87442-3165-443d-9709-960e6ddafe7f with input: \nquery: What did the author do during his time in YC?\n\nSelecting component 0: The author used a tool to answer specific questions about the document, which suggests that he was engaged in analyzing and extracting specific information from the document during his time in YC..\nDuring his time in YC, the author worked on various tasks related to running Y Combinator. This included selecting and helping founders, dealing with disputes between cofounders, figuring out when people were lying, and fighting with people who maltreated the startups. The author also worked on writing essays and internal software for YC.\n</pre> In\u00a0[\u00a0]: Copied! <pre>response = qp.run(\"What is a summary of this document?\")\nprint(str(response))\n</pre> response = qp.run(\"What is a summary of this document?\") print(str(response)) <pre>&gt; Running module c0a87442-3165-443d-9709-960e6ddafe7f with input: \nquery: What is a summary of this document?\n\nSelecting component 1: The summary questions about the document are answered by this tool..\n&gt; Running module 0e7e9d49-4c92-45a9-b3bf-0e6ab76b51f9 with input: \nquery_str: What is a summary of this document?\n\n&gt; Running module b0ece4e3-e6cd-4229-8663-b0cd0638683c with input: \nmessages: Here's a question:\nWhat is a summary of this document?\n\nYou are responsible for feeding the question to an agent that given context will try to answer the question.\nThe context may or may not be relev...\n\n&gt; Running module f247ae78-a71c-4347-ba49-d9357ee93636 with input: \ninput: assistant: What is the summary of the document?\n\nThe document discusses the development and evolution of Lisp as a programming language. It highlights how Lisp was originally created as a formal model of computation and later transformed into a programming language with the assistance of Steve Russell. The document also emphasizes the unique power and elegance of Lisp in comparison to other languages.\n</pre>"},{"location":"examples/pipeline/query_pipeline_routing/#query-pipeline-with-routing","title":"Query Pipeline with Routing\u00b6","text":"<p>Here we showcase our query pipeline with routing.</p> <p>Routing lets us dynamically choose underlying query pipelines to use given the query and a set of choices.</p> <p>We offer this as an out-of-the-box abstraction in our Router Query Engine guide. Here we show you how to compose a similar pipeline using our Query Pipeline syntax - this allows you to not only define query engines but easily stitch it into a chain/DAG with other modules across the compute graph.</p>"},{"location":"examples/pipeline/query_pipeline_routing/#load-data","title":"Load Data\u00b6","text":"<p>Load in the Paul Graham essay as an example.</p>"},{"location":"examples/pipeline/query_pipeline_routing/#setup-query-pipeline-with-routing","title":"Setup Query Pipeline with Routing\u00b6","text":""},{"location":"examples/pipeline/query_pipeline_routing/#define-modules","title":"Define Modules\u00b6","text":"<p>We define llm, vector index, summary index, and prompt templates.</p>"},{"location":"examples/pipeline/query_pipeline_routing/#construct-query-pipelines","title":"Construct Query Pipelines\u00b6","text":"<p>Define a query pipeline for vector index, summary index, and join it together with a router.</p>"},{"location":"examples/pipeline/query_pipeline_routing/#try-out-queries","title":"Try out Queries\u00b6","text":""},{"location":"examples/query_engine/RouterQueryEngine/","title":"Router Query Engine","text":"<p>If you're opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-embeddings-openai\n%pip install llama-index-llms-openai\n</pre> %pip install llama-index-embeddings-openai %pip install llama-index-llms-openai In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index\n</pre> !pip install llama-index In\u00a0[\u00a0]: Copied! <pre># NOTE: This is ONLY necessary in jupyter notebook.\n# Details: Jupyter runs an event-loop behind the scenes.\n#          This results in nested event-loops when we start an event-loop to make async queries.\n#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> # NOTE: This is ONLY necessary in jupyter notebook. # Details: Jupyter runs an event-loop behind the scenes. #          This results in nested event-loops when we start an event-loop to make async queries. #          This is normally not allowed, we use nest_asyncio to allow it for convenience. import nest_asyncio  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import Settings\n\nSettings.llm = OpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.2)\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n</pre> from llama_index.llms.openai import OpenAI from llama_index.embeddings.openai import OpenAIEmbedding from llama_index.core import Settings  Settings.llm = OpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.2) Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\") In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import SimpleDirectoryReader\n\n# load documents\ndocuments = SimpleDirectoryReader(\"../data/paul_graham\").load_data()\n</pre> from llama_index.core import SimpleDirectoryReader  # load documents documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data() In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import Settings\n\n# initialize settings (set chunk size)\nSettings.chunk_size = 1024\nnodes = Settings.node_parser.get_nodes_from_documents(documents)\n</pre> from llama_index.core import Settings  # initialize settings (set chunk size) Settings.chunk_size = 1024 nodes = Settings.node_parser.get_nodes_from_documents(documents) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import StorageContext\n\n# initialize storage context (by default it's in-memory)\nstorage_context = StorageContext.from_defaults()\nstorage_context.docstore.add_documents(nodes)\n</pre> from llama_index.core import StorageContext  # initialize storage context (by default it's in-memory) storage_context = StorageContext.from_defaults() storage_context.docstore.add_documents(nodes) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import SummaryIndex\nfrom llama_index.core import VectorStoreIndex\n\nsummary_index = SummaryIndex(nodes, storage_context=storage_context)\nvector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n</pre> from llama_index.core import SummaryIndex from llama_index.core import VectorStoreIndex  summary_index = SummaryIndex(nodes, storage_context=storage_context) vector_index = VectorStoreIndex(nodes, storage_context=storage_context) In\u00a0[\u00a0]: Copied! <pre>list_query_engine = summary_index.as_query_engine(\n    response_mode=\"tree_summarize\",\n    use_async=True,\n)\nvector_query_engine = vector_index.as_query_engine()\n</pre> list_query_engine = summary_index.as_query_engine(     response_mode=\"tree_summarize\",     use_async=True, ) vector_query_engine = vector_index.as_query_engine() In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.tools import QueryEngineTool\n\n\nlist_tool = QueryEngineTool.from_defaults(\n    query_engine=list_query_engine,\n    description=(\n        \"Useful for summarization questions related to Paul Graham eassy on\"\n        \" What I Worked On.\"\n    ),\n)\n\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=vector_query_engine,\n    description=(\n        \"Useful for retrieving specific context from Paul Graham essay on What\"\n        \" I Worked On.\"\n    ),\n)\n</pre> from llama_index.core.tools import QueryEngineTool   list_tool = QueryEngineTool.from_defaults(     query_engine=list_query_engine,     description=(         \"Useful for summarization questions related to Paul Graham eassy on\"         \" What I Worked On.\"     ), )  vector_tool = QueryEngineTool.from_defaults(     query_engine=vector_query_engine,     description=(         \"Useful for retrieving specific context from Paul Graham essay on What\"         \" I Worked On.\"     ), ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.query_engine import RouterQueryEngine\nfrom llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\nfrom llama_index.core.selectors import (\n    PydanticMultiSelector,\n    PydanticSingleSelector,\n)\n\n\nquery_engine = RouterQueryEngine(\n    selector=PydanticSingleSelector.from_defaults(),\n    query_engine_tools=[\n        list_tool,\n        vector_tool,\n    ],\n)\n</pre> from llama_index.core.query_engine import RouterQueryEngine from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector from llama_index.core.selectors import (     PydanticMultiSelector,     PydanticSingleSelector, )   query_engine = RouterQueryEngine(     selector=PydanticSingleSelector.from_defaults(),     query_engine_tools=[         list_tool,         vector_tool,     ], ) In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(\"What is the summary of the document?\")\nprint(str(response))\n</pre> response = query_engine.query(\"What is the summary of the document?\") print(str(response)) <pre>The document provides a comprehensive account of the author's diverse experiences, including writing, programming, founding and running startups, and investing in early-stage companies. It covers the challenges, successes, and lessons learned in these ventures, as well as the author's personal and professional growth, interactions with colleagues, and evolving interests and priorities over time.\n</pre> In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(\"What did Paul Graham do after RICS?\")\nprint(str(response))\n</pre> response = query_engine.query(\"What did Paul Graham do after RICS?\") print(str(response)) <pre>Paul Graham started painting after leaving Y Combinator. He wanted to see how good he could get if he really focused on it. After spending most of 2014 painting, he eventually ran out of steam and stopped working on it. He then started writing essays again and wrote a bunch of new ones over the next few months. Later, in March 2015, he started working on Lisp again.\n</pre> In\u00a0[\u00a0]: Copied! <pre>query_engine = RouterQueryEngine(\n    selector=LLMSingleSelector.from_defaults(),\n    query_engine_tools=[\n        list_tool,\n        vector_tool,\n    ],\n)\n</pre> query_engine = RouterQueryEngine(     selector=LLMSingleSelector.from_defaults(),     query_engine_tools=[         list_tool,         vector_tool,     ], ) In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(\"What is the summary of the document?\")\nprint(str(response))\n</pre> response = query_engine.query(\"What is the summary of the document?\") print(str(response)) <pre>The document provides a comprehensive account of the author's professional journey, covering his involvement in various projects such as Viaweb, Y Combinator, and Hacker News, as well as his transition to focusing on writing essays and working on Y Combinator. It also delves into his experiences with the Summer Founders Program, the growth and challenges of Y Combinator, personal struggles, and his return to working on Lisp. The author reflects on the challenges and successes encountered throughout his career, including funding startups, developing a new version of Arc, and the impact of Hacker News. Additionally, the document touches on the author's interactions with colleagues, his time in Italy, experiences with painting, and the completion of a new Lisp called Bel. Throughout, the author shares insights and lessons learned from his diverse experiences.\n</pre> In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(\"What did Paul Graham do after RICS?\")\nprint(str(response))\n</pre> response = query_engine.query(\"What did Paul Graham do after RICS?\") print(str(response)) <pre>Paul Graham started painting after leaving Y Combinator. He wanted to see how good he could get if he really focused on it. After spending most of 2014 painting, he eventually ran out of steam and stopped working on it. He then started writing essays again and wrote a bunch of new ones over the next few months. In March 2015, he started working on Lisp again.\n</pre> In\u00a0[\u00a0]: Copied! <pre># [optional] look at selected results\nprint(str(response.metadata[\"selector_result\"]))\n</pre> # [optional] look at selected results print(str(response.metadata[\"selector_result\"])) <pre>selections=[SingleSelection(index=1, reason='The question is asking for specific context about what Paul Graham did after RICS, which would require retrieving specific information from his essay.')]\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import SimpleKeywordTableIndex\n\nkeyword_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)\n\nkeyword_tool = QueryEngineTool.from_defaults(\n    query_engine=vector_query_engine,\n    description=(\n        \"Useful for retrieving specific context using keywords from Paul\"\n        \" Graham essay on What I Worked On.\"\n    ),\n)\n</pre> from llama_index.core import SimpleKeywordTableIndex  keyword_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)  keyword_tool = QueryEngineTool.from_defaults(     query_engine=vector_query_engine,     description=(         \"Useful for retrieving specific context using keywords from Paul\"         \" Graham essay on What I Worked On.\"     ), ) In\u00a0[\u00a0]: Copied! <pre>query_engine = RouterQueryEngine(\n    selector=PydanticMultiSelector.from_defaults(),\n    query_engine_tools=[\n        list_tool,\n        vector_tool,\n        keyword_tool,\n    ],\n)\n</pre> query_engine = RouterQueryEngine(     selector=PydanticMultiSelector.from_defaults(),     query_engine_tools=[         list_tool,         vector_tool,         keyword_tool,     ], ) In\u00a0[\u00a0]: Copied! <pre># This query could use either a keyword or vector query engine, so it will combine responses from both\nresponse = query_engine.query(\n    \"What were noteable events and people from the authors time at Interleaf\"\n    \" and YC?\"\n)\nprint(str(response))\n</pre> # This query could use either a keyword or vector query engine, so it will combine responses from both response = query_engine.query(     \"What were noteable events and people from the authors time at Interleaf\"     \" and YC?\" ) print(str(response)) <pre>The author's time at Interleaf involved working on software for creating documents and learning valuable lessons about what not to do. Notable individuals associated with Y Combinator during the author's time there include Jessica Livingston, Robert Morris, and Sam Altman, who eventually became the second president of YC. The author's time at Y Combinator included notable events such as the creation of the Summer Founders Program, which attracted impressive individuals like Reddit, Justin Kan, Emmett Shear, Aaron Swartz, and Sam Altman.\n</pre> In\u00a0[\u00a0]: Copied! <pre># [optional] look at selected results\nprint(str(response.metadata[\"selector_result\"]))\n</pre> # [optional] look at selected results print(str(response.metadata[\"selector_result\"])) <pre>selections=[SingleSelection(index=0, reason='Summarization questions related to Paul Graham essay on What I Worked On.'), SingleSelection(index=2, reason='Retrieving specific context using keywords from Paul Graham essay on What I Worked On.')]\n</pre>"},{"location":"examples/query_engine/RouterQueryEngine/#router-query-engine","title":"Router Query Engine\u00b6","text":"<p>In this tutorial, we define a custom router query engine that selects one out of several candidate query engines to execute a query.</p>"},{"location":"examples/query_engine/RouterQueryEngine/#setup","title":"Setup\u00b6","text":""},{"location":"examples/query_engine/RouterQueryEngine/#global-models","title":"Global Models\u00b6","text":""},{"location":"examples/query_engine/RouterQueryEngine/#load-data","title":"Load Data\u00b6","text":"<p>We first show how to convert a Document into a set of Nodes, and insert into a DocumentStore.</p>"},{"location":"examples/query_engine/RouterQueryEngine/#define-summary-index-and-vector-index-over-same-data","title":"Define Summary Index and Vector Index over Same Data\u00b6","text":""},{"location":"examples/query_engine/RouterQueryEngine/#define-query-engines-and-set-metadata","title":"Define Query Engines and Set Metadata\u00b6","text":""},{"location":"examples/query_engine/RouterQueryEngine/#define-router-query-engine","title":"Define Router Query Engine\u00b6","text":"<p>There are several selectors available, each with some distinct attributes.</p> <p>The LLM selectors use the LLM to output a JSON that is parsed, and the corresponding indexes are queried.</p> <p>The Pydantic selectors (currently only supported by <code>gpt-4-0613</code> and <code>gpt-3.5-turbo-0613</code> (the default)) use the OpenAI Function Call API to produce pydantic selection objects, rather than parsing raw JSON.</p> <p>For each type of selector, there is also the option to select 1 index to route to, or multiple.</p>"},{"location":"examples/query_engine/RouterQueryEngine/#pydanticsingleselector","title":"PydanticSingleSelector\u00b6","text":"<p>Use the OpenAI Function API to generate/parse pydantic objects under the hood for the router selector.</p>"},{"location":"examples/query_engine/RouterQueryEngine/#llmsingleselector","title":"LLMSingleSelector\u00b6","text":"<p>Use OpenAI (or any other LLM) to parse generated JSON under the hood to select a sub-index for routing.</p>"},{"location":"examples/query_engine/RouterQueryEngine/#pydanticmultiselector","title":"PydanticMultiSelector\u00b6","text":"<p>In case you are expecting queries to be routed to multiple indexes, you should use a multi selector. The multi selector sends to query to multiple sub-indexes, and then aggregates all responses using a summary index to form a complete answer.</p>"},{"location":"examples/query_engine/pandas_query_engine/","title":"Pandas Query Engine","text":"<p>If you're opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index llama-index-experimental\n</pre> !pip install llama-index llama-index-experimental In\u00a0[\u00a0]: Copied! <pre>import logging\nimport sys\nfrom IPython.display import Markdown, display\n\nimport pandas as pd\nfrom llama_index.experimental.query_engine import PandasQueryEngine\n\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n</pre> import logging import sys from IPython.display import Markdown, display  import pandas as pd from llama_index.experimental.query_engine import PandasQueryEngine   logging.basicConfig(stream=sys.stdout, level=logging.INFO) logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout)) In\u00a0[\u00a0]: Copied! <pre># Test on some sample data\ndf = pd.DataFrame(\n    {\n        \"city\": [\"Toronto\", \"Tokyo\", \"Berlin\"],\n        \"population\": [2930000, 13960000, 3645000],\n    }\n)\n</pre> # Test on some sample data df = pd.DataFrame(     {         \"city\": [\"Toronto\", \"Tokyo\", \"Berlin\"],         \"population\": [2930000, 13960000, 3645000],     } ) In\u00a0[\u00a0]: Copied! <pre>query_engine = PandasQueryEngine(df=df, verbose=True)\n</pre> query_engine = PandasQueryEngine(df=df, verbose=True) In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(\n    \"What is the city with the highest population?\",\n)\n</pre> response = query_engine.query(     \"What is the city with the highest population?\", ) <pre>INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nHTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n&gt; Pandas Instructions:\n```\ndf['city'][df['population'].idxmax()]\n```\n&gt; Pandas Output: Tokyo\n</pre> In\u00a0[\u00a0]: Copied! <pre>display(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n</pre> display(Markdown(f\"{response}\")) <p>Tokyo</p> In\u00a0[\u00a0]: Copied! <pre># get pandas python instructions\nprint(response.metadata[\"pandas_instruction_str\"])\n</pre> # get pandas python instructions print(response.metadata[\"pandas_instruction_str\"]) <pre>df['city'][df['population'].idxmax()]\n</pre> <p>We can also take the step of using an LLM to synthesize a response.</p> In\u00a0[\u00a0]: Copied! <pre>query_engine = PandasQueryEngine(df=df, verbose=True, synthesize_response=True)\nresponse = query_engine.query(\n    \"What is the city with the highest population? Give both the city and population\",\n)\nprint(str(response))\n</pre> query_engine = PandasQueryEngine(df=df, verbose=True, synthesize_response=True) response = query_engine.query(     \"What is the city with the highest population? Give both the city and population\", ) print(str(response)) <pre>INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nHTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n&gt; Pandas Instructions:\n```\ndf.loc[df['population'].idxmax()]\n```\n&gt; Pandas Output: city             Tokyo\npopulation    13960000\nName: 1, dtype: object\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nHTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nThe city with the highest population is Tokyo, with a population of 13,960,000.\n</pre> In\u00a0[\u00a0]: Copied! <pre>!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/docs/examples/data/csv/titanic_train.csv' -O 'titanic_train.csv'\n</pre> !wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/docs/examples/data/csv/titanic_train.csv' -O 'titanic_train.csv' <pre>--2024-01-13 17:45:15--  https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/docs/examples/data/csv/titanic_train.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8002::154, 2606:50c0:8001::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 57726 (56K) [text/plain]\nSaving to: \u2018titanic_train.csv\u2019\n\ntitanic_train.csv   100%[===================&gt;]  56.37K  --.-KB/s    in 0.009s  \n\n2024-01-13 17:45:15 (6.45 MB/s) - \u2018titanic_train.csv\u2019 saved [57726/57726]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv(\"./titanic_train.csv\")\n</pre> df = pd.read_csv(\"./titanic_train.csv\") In\u00a0[\u00a0]: Copied! <pre>query_engine = PandasQueryEngine(df=df, verbose=True)\n</pre> query_engine = PandasQueryEngine(df=df, verbose=True) In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(\n    \"What is the correlation between survival and age?\",\n)\n</pre> response = query_engine.query(     \"What is the correlation between survival and age?\", ) <pre>INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nHTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n&gt; Pandas Instructions:\n```\ndf['survived'].corr(df['age'])\n```\n&gt; Pandas Output: -0.07722109457217755\n</pre> In\u00a0[\u00a0]: Copied! <pre>display(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n</pre> display(Markdown(f\"{response}\")) <p>-0.07722109457217755</p> In\u00a0[\u00a0]: Copied! <pre># get pandas python instructions\nprint(response.metadata[\"pandas_instruction_str\"])\n</pre> # get pandas python instructions print(response.metadata[\"pandas_instruction_str\"]) <pre>df['survived'].corr(df['age'])\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import PromptTemplate\n</pre> from llama_index.core import PromptTemplate In\u00a0[\u00a0]: Copied! <pre>query_engine = PandasQueryEngine(df=df, verbose=True)\nprompts = query_engine.get_prompts()\nprint(prompts[\"pandas_prompt\"].template)\n</pre> query_engine = PandasQueryEngine(df=df, verbose=True) prompts = query_engine.get_prompts() print(prompts[\"pandas_prompt\"].template) <pre>You are working with a pandas dataframe in Python.\nThe name of the dataframe is `df`.\nThis is the result of `print(df.head())`:\n{df_str}\n\nFollow these instructions:\n{instruction_str}\nQuery: {query_str}\n\nExpression:\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(prompts[\"response_synthesis_prompt\"].template)\n</pre> print(prompts[\"response_synthesis_prompt\"].template) <pre>Given an input question, synthesize a response from the query results.\nQuery: {query_str}\n\nPandas Instructions (optional):\n{pandas_instructions}\n\nPandas Output: {pandas_output}\n\nResponse: \n</pre> <p>You can update prompts as well:</p> In\u00a0[\u00a0]: Copied! <pre>new_prompt = PromptTemplate(\n    \"\"\"\\\nYou are working with a pandas dataframe in Python.\nThe name of the dataframe is `df`.\nThis is the result of `print(df.head())`:\n{df_str}\n\nFollow these instructions:\n{instruction_str}\nQuery: {query_str}\n\nExpression: \"\"\"\n)\n\nquery_engine.update_prompts({\"pandas_prompt\": new_prompt})\n</pre> new_prompt = PromptTemplate(     \"\"\"\\ You are working with a pandas dataframe in Python. The name of the dataframe is `df`. This is the result of `print(df.head())`: {df_str}  Follow these instructions: {instruction_str} Query: {query_str}  Expression: \"\"\" )  query_engine.update_prompts({\"pandas_prompt\": new_prompt}) <p>This is the instruction string (that you can customize by passing in <code>instruction_str</code> on initialization)</p> In\u00a0[\u00a0]: Copied! <pre>instruction_str = \"\"\"\\\n1. Convert the query to executable Python code using Pandas.\n2. The final line of code should be a Python expression that can be called with the `eval()` function.\n3. The code should represent a solution to the query.\n4. PRINT ONLY THE EXPRESSION.\n5. Do not quote the expression.\n\"\"\"\n</pre> instruction_str = \"\"\"\\ 1. Convert the query to executable Python code using Pandas. 2. The final line of code should be a Python expression that can be called with the `eval()` function. 3. The code should represent a solution to the query. 4. PRINT ONLY THE EXPRESSION. 5. Do not quote the expression. \"\"\""},{"location":"examples/query_engine/pandas_query_engine/#pandas-query-engine","title":"Pandas Query Engine\u00b6","text":"<p>This guide shows you how to use our <code>PandasQueryEngine</code>: convert natural language to Pandas python code using LLMs.</p> <p>The input to the <code>PandasQueryEngine</code> is a Pandas dataframe, and the output is a response. The LLM infers dataframe operations to perform in order to retrieve the result.</p> <p>WARNING: This tool provides the LLM access to the <code>eval</code> function. Arbitrary code execution is possible on the machine running this tool. While some level of filtering is done on code, this tool is not recommended to be used in a production setting without heavy sandboxing or virtual machines.</p>"},{"location":"examples/query_engine/pandas_query_engine/#lets-start-on-a-toy-dataframe","title":"Let's start on a Toy DataFrame\u00b6","text":"<p>Here let's load a very simple dataframe containing city and population pairs, and run the <code>PandasQueryEngine</code> on it.</p> <p>By setting <code>verbose=True</code> we can see the intermediate generated instructions.</p>"},{"location":"examples/query_engine/pandas_query_engine/#analyzing-the-titanic-dataset","title":"Analyzing the Titanic Dataset\u00b6","text":"<p>The Titanic dataset is one of the most popular tabular datasets in introductory machine learning Source: https://www.kaggle.com/c/titanic</p>"},{"location":"examples/query_engine/pandas_query_engine/#download-data","title":"Download Data\u00b6","text":""},{"location":"examples/query_engine/pandas_query_engine/#additional-steps","title":"Additional Steps\u00b6","text":""},{"location":"examples/query_engine/pandas_query_engine/#analyzing-modifying-prompts","title":"Analyzing / Modifying prompts\u00b6","text":"<p>Let's look at the prompts!</p>"},{"location":"examples/query_engine/pandas_query_engine/#implementing-query-engine-using-query-pipeline-syntax","title":"Implementing Query Engine using Query Pipeline Syntax\u00b6","text":"<p>If you want to learn to construct your own Pandas Query Engine using our Query Pipeline syntax and the prompt components above, check out our below tutorial.</p> <p>Setting up a Pandas DataFrame query engine with Query Pipelines</p>"}]}